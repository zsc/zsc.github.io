<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MNIST U-Net Diffusion Demo (Pause/Resume)</title>
    <style>
        body { font-family: sans-serif; margin: 20px; background-color: #f4f4f4; color: #333; }
        .container { background-color: #fff; padding: 20px; border-radius: 8px; box-shadow: 0 0 10px rgba(0,0,0,0.1); }
        h1, h2 { color: #333; }
        label { display: inline-block; width: 180px; margin-bottom: 5px; }
        input[type="number"], input[type="text"] { width: 80px; padding: 5px; margin-bottom: 10px; border: 1px solid #ddd; border-radius: 4px; }
        button { padding: 10px 15px; background-color: #007bff; color: white; border: none; border-radius: 4px; cursor: pointer; margin-right: 10px; margin-bottom: 5px; }
        button:hover { background-color: #0056b3; }
        button:disabled { background-color: #ccc; }
        #log { width: 100%; height: 200px; overflow-y: scroll; border: 1px solid #ddd; padding: 10px; box-sizing: border-box; background-color: #e9e9e9; white-space: pre-wrap; margin-top: 15px; }
        .canvases { display: flex; flex-wrap: wrap; gap: 10px; margin-top: 20px; }
        canvas { border: 1px solid black; image-rendering: pixelated; image-rendering: -moz-crisp-edges; image-rendering: crisp-edges; }
        .canvas-container { text-align: center; }
        .canvas-container p { margin-bottom: 5px; font-size: 0.9em; }
    </style>
</head>
<body>
    <div class="container">
        <h1>MNIST Diffusion Denoising Demo (U-Net Pause/Resume)</h1>

        <h2>Hyperparameters</h2>
        <div><label for="numTrainImages">Number of Images (max 60k):</label><input type="number" id="numTrainImages" value="100"></div>
        <div><label for="epochs">Epochs:</label><input type="number" id="epochs" value="3"></div> <!-- Slightly more epochs to see pause benefit -->
        <div><label for="learningRate">Learning Rate:</label><input type="text" id="learningRate" value="0.00005"></div>
        <div><label for="timestepsT">Diffusion Timesteps (T):</label><input type="number" id="timestepsT" value="20"></div>
        <div><label for="betaStart">Beta Start:</label><input type="text" id="betaStart" value="0.0001"></div>
        <div><label for="betaEnd">Beta End:</label><input type="text" id="betaEnd" value="0.02"></div>

        <button id="trainButton">Start Training</button>
        <button id="pauseResumeButton" disabled>Pause Training</button>
        <button id="generateButton" disabled>Generate Sample from Noise</button>
        <button id="testDenoiseButton" disabled>Test Denoising Random Image</button>

        <h2>Log</h2>
        <div id="log">Welcome! U-Net version with Pause/Resume. Model size: baseFilters=2, timeEmbedDim=4.<br>Image data will be fetched automatically.</div>

        <h2>Visualizations</h2>
        <div class="canvases">
            <div class="canvas-container"><p>Original (x_0)</p><canvas id="originalCanvas" width="112" height="112"></canvas></div>
            <div class="canvas-container"><p>Noisy (x_t)</p><canvas id="noisyCanvas" width="112" height="112"></canvas></div>
            <div class="canvas-container"><p>Predicted Noise (ε_θ)</p><canvas id="predictedNoiseCanvas" width="112" height="112"></canvas></div>
            <div class="canvas-container"><p>Denoised (from x_t)</p><canvas id="denoisedStepCanvas" width="112" height="112"></canvas></div>
            <div class="canvas-container"><p>Generated Sample</p><canvas id="generatedCanvas" width="112" height="112"></canvas></div>
        </div>
    </div>

    <script>
        // --- Global Constants & UI Elements ---
        const IMAGE_SIZE = 28;
        const STITCHED_IMAGE_URL = 'https://zsc.github.io/widgets/mnist/mnist_train_stitched.png';
        const IMAGES_PER_ROW_STITCHED = 245;

        const logElement = document.getElementById('log');
        const trainButton = document.getElementById('trainButton');
        const pauseResumeButton = document.getElementById('pauseResumeButton');
        const generateButton = document.getElementById('generateButton');
        const testDenoiseButton = document.getElementById('testDenoiseButton');

        let mnistData = [];
        let model = null;
        let diffusionParams = {};
        
        // --- Training State ---
        let isTrainingActive = false;
        let isTrainingPaused = false;
        let currentEpoch = 0;
        let currentImageIndex = 0;
        let totalEpochsGoal = 0;
        let totalImagesPerEpoch = 0;
        let learningRateVal = 0.00005; // Store parsed LR

        let resolvePause = null; // Used to unblock the training loop when resuming

        let validLossCount = 0;
        let totalLossSinceStart = 0; // Accumulates loss across all valid steps since training started/restarted

        function log(message) {
            logElement.innerHTML += message + '<br>';
            logElement.scrollTop = logElement.scrollHeight;
            console.log(message);
        }

        // --- MNIST Data Handling (Identical) ---
        async function loadAndParseMNIST(numImagesToLoad) { /* ... (Identical to previous fixed version) ... */ 
            log('Loading MNIST stitched image...');
            const stitchedImg = new Image();
            stitchedImg.crossOrigin = "anonymous";
            
            return new Promise((resolve, reject) => {
                stitchedImg.onload = () => {
                    log('MNIST image loaded. Parsing...');
                    const canvas = document.createElement('canvas');
                    canvas.width = stitchedImg.width;
                    canvas.height = stitchedImg.height;
                    const ctx = canvas.getContext('2d');
                    ctx.drawImage(stitchedImg, 0, 0);
                    const imageData = ctx.getImageData(0, 0, stitchedImg.width, stitchedImg.height).data;

                    mnistData = [];
                    for (let i = 0; i < numImagesToLoad; i++) {
                        const img = new Float32Array(IMAGE_SIZE * IMAGE_SIZE);
                        const rowInStitched = Math.floor(i / IMAGES_PER_ROW_STITCHED);
                        const colInStitched = i % IMAGES_PER_ROW_STITCHED;
                        const startX = colInStitched * IMAGE_SIZE;
                        const startY = rowInStitched * IMAGE_SIZE;

                        for (let y = 0; y < IMAGE_SIZE; y++) {
                            for (let x = 0; x < IMAGE_SIZE; x++) {
                                const stitchedX = startX + x;
                                const stitchedY = startY + y;
                                const pixelIndex = (stitchedY * stitchedImg.width + stitchedX) * 4;
                                const grayValue = imageData[pixelIndex];
                                img[y * IMAGE_SIZE + x] = (grayValue / 255.0) * 2.0 - 1.0;
                            }
                        }
                        mnistData.push(img);
                        if ((i + 1) % Math.max(1, Math.floor(numImagesToLoad / 10)) === 0 && i < numImagesToLoad -1) {
                             log(`Parsed ${i + 1}/${numImagesToLoad} images...`);
                        }
                    }
                    log(`Successfully parsed ${mnistData.length} MNIST images.`);
                    resolve();
                };
                stitchedImg.onerror = (err) => { log('Error loading MNIST image: ' + err); reject(err); };
                stitchedImg.src = STITCHED_IMAGE_URL;
            });
        }

        function drawToCanvas(canvasId, flatImageData, width = IMAGE_SIZE, height = IMAGE_SIZE, scaleFactor = 4) { /* ... (Identical) ... */ 
            const canvas = document.getElementById(canvasId);
            const ctx = canvas.getContext('2d');
            canvas.width = width * scaleFactor;
            canvas.height = height * scaleFactor;
            ctx.clearRect(0,0,canvas.width, canvas.height);
            if (flatImageData && flatImageData.length === width * height) {
                const imgData = ctx.createImageData(width, height);
                for (let i = 0; i < flatImageData.length; i++) {
                    const val = Math.round((flatImageData[i] + 1.0) / 2.0 * 255.0);
                    const safeVal = Math.max(0, Math.min(255, val));
                    imgData.data[i * 4 + 0] = safeVal;
                    imgData.data[i * 4 + 1] = safeVal;
                    imgData.data[i * 4 + 2] = safeVal;
                    imgData.data[i * 4 + 3] = 255;
                }
                const tempCanvas = document.createElement('canvas');
                tempCanvas.width = width; tempCanvas.height = height;
                tempCanvas.getContext('2d').putImageData(imgData, 0, 0);
                ctx.imageSmoothingEnabled = false;
                ctx.drawImage(tempCanvas, 0, 0, width * scaleFactor, height * scaleFactor);
            }
        }
        
        // --- Math & NN Helpers (Tensor, Activations, Layers - Identical to previous fixed version) ---
        function generateGaussianNoise(size) { /* ... (Identical) ... */ 
            const noise = new Float32Array(size);
            for (let i = 0; i < size; i += 2) {
                let u1 = 0, u2 = 0;
                while (u1 === 0) u1 = Math.random();
                while (u2 === 0) u2 = Math.random();
                const R = Math.sqrt(-2.0 * Math.log(u1));
                const Theta = 2.0 * Math.PI * u2;
                noise[i] = R * Math.cos(Theta);
                if (i + 1 < size) noise[i + 1] = R * Math.sin(Theta);
            }
            return noise;
        }
        class Tensor { /* ... (Identical to previous fixed version) ... */ 
            constructor(shape, data) {
                this.shape = shape;
                const size = shape.reduce((a, b) => a * b, 1);
                this.data = data instanceof Float32Array ? data : new Float32Array(size);
                if (data && !(data instanceof Float32Array) && data.length === size) { 
                    this.data.set(data);
                } else if (data && data.length !== size) {
                     throw new Error(`Data length ${data.length} does not match shape size ${size}`);
                }
            }
            _getIndex(indices) { 
                if (indices.length !== this.shape.length) {
                    throw new Error(`Dimension mismatch for indices: expected ${this.shape.length}, got ${indices.length}. Shape: ${this.shape}, Indices: ${indices}`);
                }
                let flatIndex = 0;
                if (this.shape.length === 1) { 
                    flatIndex = indices[0];
                } else if (this.shape.length === 2) { 
                    flatIndex = indices[0] * this.shape[1] + indices[1];
                } else if (this.shape.length === 3) { 
                    flatIndex = indices[0] * this.shape[1] * this.shape[2] + 
                                indices[1] * this.shape[2] + 
                                indices[2];
                } else if (this.shape.length === 4) { 
                    flatIndex = indices[0] * this.shape[1] * this.shape[2] * this.shape[3] +
                                indices[1] * this.shape[2] * this.shape[3] +
                                indices[2] * this.shape[3] +
                                indices[3];
                } else {
                    throw new Error(`Tensor get/set for ${this.shape.length}D not implemented yet (max 4D supported). Shape: ${this.shape}`);
                }
                if (flatIndex < 0 || flatIndex >= this.data.length) {
                     throw new Error(`Calculated flatIndex ${flatIndex} out of bounds (0-${this.data.length-1}). Shape: ${this.shape}, Indices: ${indices}`);
                }
                return flatIndex;
            }
            get(indicesOrSingle) {
                if (!Array.isArray(indicesOrSingle)) { 
                    if (this.shape.length === 1 && (indicesOrSingle < 0 || indicesOrSingle >= this.shape[0])) {
                         throw new Error(`Index ${indicesOrSingle} out of bounds for 1D Tensor of size ${this.shape[0]}`);
                    }
                    if (this.shape.length > 1 && (indicesOrSingle < 0 || indicesOrSingle >= this.data.length)) {
                        throw new Error(`Flat index ${indicesOrSingle} out of bounds for Tensor data of size ${this.data.length}`);
                    }
                    return this.data[indicesOrSingle];
                }
                return this.data[this._getIndex(indicesOrSingle)];
            }
            set(indicesOrSingle, value) {
                if (!Array.isArray(indicesOrSingle)) { 
                    if (this.shape.length === 1 && (indicesOrSingle < 0 || indicesOrSingle >= this.shape[0])) {
                         throw new Error(`Index ${indicesOrSingle} out of bounds for 1D Tensor of size ${this.shape[0]}`);
                    }
                     if (this.shape.length > 1 && (indicesOrSingle < 0 || indicesOrSingle >= this.data.length)) {
                        throw new Error(`Flat index ${indicesOrSingle} out of bounds for Tensor data of size ${this.data.length}`);
                    }
                    this.data[indicesOrSingle] = value;
                    return;
                }
                this.data[this._getIndex(indicesOrSingle)] = value;
            }
            clone() { return new Tensor([...this.shape], new Float32Array(this.data)); }
            fill(value) { this.data.fill(value); return this; }
            getCHW(c,h,w) { return this.data[c * this.shape[1]*this.shape[2] + h * this.shape[2] + w]; }
            setCHW(c,h,w,val) { this.data[c * this.shape[1]*this.shape[2] + h * this.shape[2] + w] = val; }
            static fromFlat(flatData, H, W, C=1) {
                const tensor = new Tensor([C,H,W]);
                if (C === 1 && flatData.length === H*W) {
                    for(let i=0; i<H*W; ++i) tensor.data[i] = flatData[i];
                } else if (flatData.length === C*H*W) {
                    tensor.data.set(flatData);
                } else {
                    throw new Error(`fromFlat data length mismatch. Data: ${flatData.length}, Expected C*H*W: ${C*H*W}`);
                }
                return tensor;
            }
            toFlat() {
                if (this.shape[0] > 1 && this.shape.length === 3) { 
                    const firstChannelSize = this.shape[1] * this.shape[2];
                    return this.data.slice(0, firstChannelSize);
                }
                return this.data; 
            }
        }
        class ReLULayer { /* ... (Identical) ... */ 
            forward(inputTensor) {
                this.inputTensor = inputTensor;
                const outputTensor = inputTensor.clone();
                for (let i = 0; i < outputTensor.data.length; i++) {
                    if (outputTensor.data[i] < 0) outputTensor.data[i] = 0;
                }
                return outputTensor;
            }
            backward(dOutputTensor) {
                const dInputTensor = dOutputTensor.clone();
                for (let i = 0; i < this.inputTensor.data.length; i++) {
                    if (this.inputTensor.data[i] <= 0) dInputTensor.data[i] = 0;
                }
                return dInputTensor;
            }
        }
        class TanhLayer { /* ... (Identical) ... */
            forward(inputTensor) {
                this.outputTensor = inputTensor.clone(); 
                for (let i = 0; i < this.outputTensor.data.length; i++) {
                    this.outputTensor.data[i] = Math.tanh(this.outputTensor.data[i]);
                }
                return this.outputTensor;
            }
            backward(dOutputTensor) {
                const dInputTensor = dOutputTensor.clone();
                for (let i = 0; i < this.outputTensor.data.length; i++) {
                    const tanh_y = this.outputTensor.data[i];
                    dInputTensor.data[i] *= (1 - tanh_y * tanh_y);
                }
                return dInputTensor;
            }
        }
        class DenseLayer { /* ... (Identical) ... */
            constructor(inputSize, outputSize) {
                this.inputSize = inputSize;
                this.outputSize = outputSize;
                const flatWeights = new Float32Array(outputSize * inputSize).map(
                    () => (Math.random() - 0.5) * 2 * Math.sqrt(1 / inputSize)
                );
                this.weights = new Tensor([outputSize, inputSize], flatWeights);
                this.biases = new Tensor([outputSize]).fill(0);
            }
            forward(inputTensor1D) {
                this.inputTensor1D = inputTensor1D; 
                const outputTensor1D = new Tensor([this.outputSize]);
                for (let o = 0; o < this.outputSize; o++) {
                    let sum = this.biases.get(o); 
                    for (let i = 0; i < this.inputSize; i++) {
                        sum += inputTensor1D.get(i) * this.weights.get([o, i]); 
                    }
                    outputTensor1D.set(o, sum); 
                }
                return outputTensor1D;
            }
            backward(dOutputTensor1D, learningRate) {
                const dInputTensor1D = new Tensor([this.inputSize]).fill(0);
                for (let o = 0; o < this.outputSize; o++) {
                    const error_grad = dOutputTensor1D.get(o);
                    for (let i = 0; i < this.inputSize; i++) {
                        const weight_grad = error_grad * this.inputTensor1D.get(i);
                        const current_weight = this.weights.get([o,i]); // Get before update for dInput calc
                        this.weights.set([o,i], current_weight - learningRate * weight_grad);
                        
                        const current_dInput = dInputTensor1D.get(i);
                        dInputTensor1D.set(i, current_dInput + error_grad * current_weight); // Use weight before update
                    }
                    this.biases.set(o, this.biases.get(o) - learningRate * error_grad);
                }
                return dInputTensor1D;
            }
        }
        class Conv2DLayer { /* ... (Identical) ... */
            constructor(inChannels, outChannels, kernelSize = 3, padding = 1, stride = 1) {
                this.inChannels = inChannels;
                this.outChannels = outChannels;
                this.kernelSize = kernelSize;
                this.padding = padding;
                this.stride = stride;

                const fanIn = inChannels * kernelSize * kernelSize;
                const fanOut = outChannels * kernelSize * kernelSize;
                const std = Math.sqrt(2.0 / (fanIn + fanOut || 1)); 

                const flatWeights = new Float32Array(outChannels * inChannels * kernelSize * kernelSize).map(
                    () => (Math.random() - 0.5) * 2 * std
                );
                this.weights = new Tensor([outChannels, inChannels, kernelSize, kernelSize], flatWeights); 
                this.biases = new Tensor([outChannels]).fill(0); 
            }
            forward(inputTensor) { 
                this.inputTensor = inputTensor;
                const [C_in, H_in, W_in] = inputTensor.shape;
                const H_out = Math.floor((H_in - this.kernelSize + 2 * this.padding) / this.stride) + 1;
                const W_out = Math.floor((W_in - this.kernelSize + 2 * this.padding) / this.stride) + 1;
                const outputTensor = new Tensor([this.outChannels, H_out, W_out]).fill(0); 

                for (let c_out = 0; c_out < this.outChannels; c_out++) {
                    for (let y_out = 0; y_out < H_out; y_out++) {
                        for (let x_out = 0; x_out < W_out; x_out++) {
                            let sum = this.biases.get(c_out); 
                            const y_start = y_out * this.stride - this.padding;
                            const x_start = x_out * this.stride - this.padding;

                            for (let c_in = 0; c_in < this.inChannels; c_in++) {
                                for (let ky = 0; ky < this.kernelSize; ky++) {
                                    for (let kx = 0; kx < this.kernelSize; kx++) {
                                        const y_in = y_start + ky;
                                        const x_in = x_start + kx;
                                        if (y_in >= 0 && y_in < H_in && x_in >= 0 && x_in < W_in) {
                                            sum += inputTensor.getCHW(c_in, y_in, x_in) *  
                                                   this.weights.get([c_out, c_in, ky, kx]); 
                                        }
                                    }
                                }
                            }
                            outputTensor.setCHW(c_out, y_out, x_out, sum); 
                        }
                    }
                }
                return outputTensor;
            }
            backward(dOutputTensor, learningRate) {
                const [C_in, H_in, W_in] = this.inputTensor.shape;
                const [C_out, H_out, W_out] = dOutputTensor.shape;
                
                const dInputTensor = new Tensor(this.inputTensor.shape).fill(0);

                for (let c_out = 0; c_out < C_out; c_out++) {
                    let sum_grad_bias = 0;
                    for (let y = 0; y < H_out; y++) {
                        for (let x = 0; x < W_out; x++) {
                            sum_grad_bias += dOutputTensor.getCHW(c_out, y, x);
                        }
                    }
                    this.biases.set(c_out, this.biases.get(c_out) - learningRate * sum_grad_bias);
                }

                for (let c_out = 0; c_out < C_out; c_out++) {
                    for (let y_out = 0; y_out < H_out; y_out++) {
                        for (let x_out = 0; x_out < W_out; x_out++) {
                            const grad_out = dOutputTensor.getCHW(c_out, y_out, x_out);
                            const y_start = y_out * this.stride - this.padding;
                            const x_start = x_out * this.stride - this.padding;

                            for (let c_in = 0; c_in < C_in; c_in++) {
                                for (let ky = 0; ky < this.kernelSize; ky++) {
                                    for (let kx = 0; kx < this.kernelSize; kx++) {
                                        const y_in = y_start + ky;
                                        const x_in = x_start + kx;

                                        if (y_in >= 0 && y_in < H_in && x_in >= 0 && x_in < W_in) {
                                            const current_weight_val = this.weights.get([c_out, c_in, ky, kx]); // Get before update
                                            const weight_grad = grad_out * this.inputTensor.getCHW(c_in, y_in, x_in);
                                            this.weights.set([c_out, c_in, ky, kx], current_weight_val - learningRate * weight_grad);
                                            
                                            const current_dInput = dInputTensor.getCHW(c_in, y_in, x_in);
                                            dInputTensor.setCHW(c_in, y_in, x_in, current_dInput + grad_out * current_weight_val); // Use weight before update
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
                return dInputTensor; 
            }
        }
        class MaxPool2DLayer { /* ... (Identical) ... */ 
            constructor(kernelSize = 2, stride = 2) {
                this.kernelSize = kernelSize;
                this.stride = stride;
            }
            forward(inputTensor) { 
                this.inputTensor = inputTensor;
                const [C, H_in, W_in] = inputTensor.shape;
                const H_out = Math.floor((H_in - this.kernelSize) / this.stride) + 1;
                const W_out = Math.floor((W_in - this.kernelSize) / this.stride) + 1;
                const outputTensor = new Tensor([C, H_out, W_out]);
                this.maxIndices = new Array(C * H_out * W_out * 2); 

                for (let c = 0; c < C; c++) {
                    for (let y = 0; y < H_out; y++) {
                        for (let x = 0; x < W_out; x++) {
                            let maxVal = -Infinity;
                            let maxY_in = -1, maxX_in = -1;
                            const y_start = y * this.stride;
                            const x_start = x * this.stride;
                            for (let ky = 0; ky < this.kernelSize; ky++) {
                                for (let kx = 0; kx < this.kernelSize; kx++) {
                                    const val = inputTensor.getCHW(c, y_start + ky, x_start + kx);
                                    if (val > maxVal) {
                                        maxVal = val;
                                        maxY_in = y_start + ky;
                                        maxX_in = x_start + kx;
                                    }
                                }
                            }
                            outputTensor.setCHW(c, y, x, maxVal);
                            const outIdxBase = (c * H_out * W_out + y * W_out + x) * 2;
                            this.maxIndices[outIdxBase] = maxY_in;
                            this.maxIndices[outIdxBase+1] = maxX_in;
                        }
                    }
                }
                return outputTensor;
            }
            backward(dOutputTensor) { 
                const [C, H_out, W_out] = dOutputTensor.shape;
                const dInputTensor = new Tensor(this.inputTensor.shape).fill(0);
                for (let c = 0; c < C; c++) {
                    for (let y = 0; y < H_out; y++) {
                        for (let x = 0; x < W_out; x++) {
                            const grad_val = dOutputTensor.getCHW(c, y, x);
                            const outIdxBase = (c * H_out * W_out + y * W_out + x) * 2;
                            const maxY_in = this.maxIndices[outIdxBase];
                            const maxX_in = this.maxIndices[outIdxBase+1];
                             if (maxY_in !== -1 && maxX_in !== -1) { 
                                const current_dInput = dInputTensor.getCHW(c, maxY_in, maxX_in);
                                dInputTensor.setCHW(c, maxY_in, maxX_in, current_dInput + grad_val);
                            }
                        }
                    }
                }
                return dInputTensor;
            }
        }
        class UpsampleNearestLayer { /* ... (Identical) ... */ 
            constructor(scaleFactor = 2) { this.scaleFactor = scaleFactor; }
            forward(inputTensor) { 
                this.inputShape = inputTensor.shape;
                const [C, H_in, W_in] = inputTensor.shape;
                const H_out = H_in * this.scaleFactor;
                const W_out = W_in * this.scaleFactor;
                const outputTensor = new Tensor([C, H_out, W_out]);
                for (let c = 0; c < C; c++) {
                    for (let y = 0; y < H_out; y++) {
                        for (let x = 0; x < W_out; x++) {
                            const y_in = Math.floor(y / this.scaleFactor);
                            const x_in = Math.floor(x / this.scaleFactor);
                            outputTensor.setCHW(c, y, x, inputTensor.getCHW(c, y_in, x_in));
                        }
                    }
                }
                return outputTensor;
            }
            backward(dOutputTensor) { 
                const [C, H_in, W_in] = this.inputShape;
                const dInputTensor = new Tensor(this.inputShape).fill(0);
                for (let c = 0; c < C; c++) {
                    for (let y_out = 0; y_out < dOutputTensor.shape[1]; y_out++) {
                        for (let x_out = 0; x_out < dOutputTensor.shape[2]; x_out++) {
                            const y_in = Math.floor(y_out / this.scaleFactor);
                            const x_in = Math.floor(x_out / this.scaleFactor);
                            if (y_in < H_in && x_in < W_in) { 
                                const current_dInput = dInputTensor.getCHW(c,y_in,x_in);
                                dInputTensor.setCHW(c, y_in, x_in, current_dInput + dOutputTensor.getCHW(c,y_out,x_out));
                            }
                        }
                    }
                }
                return dInputTensor;
            }
        }
        class ConcatLayer { /* ... (Identical) ... */
            forward(tensorA, tensorB) {
                this.shapeA = tensorA.shape; this.shapeB = tensorB.shape;
                if (tensorA.shape[1] !== tensorB.shape[1] || tensorA.shape[2] !== tensorB.shape[2]) {
                    throw new Error(`Concat spatial dim mismatch: ${tensorA.shape} vs ${tensorB.shape}`);
                }
                const [C_A, H, W] = tensorA.shape;
                const C_B = tensorB.shape[0];
                const outputTensor = new Tensor([C_A + C_B, H, W]);
                for(let c=0; c<C_A; ++c) for(let y=0; y<H; ++y) for(let x=0; x<W; ++x)
                    outputTensor.setCHW(c,y,x, tensorA.getCHW(c,y,x));
                for(let c=0; c<C_B; ++c) for(let y=0; y<H; ++y) for(let x=0; x<W; ++x)
                    outputTensor.setCHW(C_A + c, y, x, tensorB.getCHW(c,y,x));
                return outputTensor;
            }
            backward(dOutputTensor) {
                const [C_A, H, W] = this.shapeA;
                const C_B = this.shapeB[0];
                const dTensorA = new Tensor(this.shapeA);
                const dTensorB = new Tensor(this.shapeB);
                for(let c=0; c<C_A; ++c) for(let y=0; y<H; ++y) for(let x=0; x<W; ++x)
                    dTensorA.setCHW(c,y,x, dOutputTensor.getCHW(c,y,x));
                for(let c=0; c<C_B; ++c) for(let y=0; y<H; ++y) for(let x=0; x<W; ++x)
                    dTensorB.setCHW(c,y,x, dOutputTensor.getCHW(C_A + c,y,x));
                return [dTensorA, dTensorB];
            }
        }
        class TimeBroadcastAddLayer { /* ... (Identical) ... */
             forward(input_spatial, time_features_1D) { 
                this.input_spatial_shape = input_spatial.shape;
                this.time_features_1D = time_features_1D; 
                const [C, H, W] = input_spatial.shape;
                if (C !== time_features_1D.shape[0]) throw new Error(`Channel mismatch in TimeBroadcastAddLayer: Spatial C=${C}, TimeFeat C=${time_features_1D.shape[0]}`);
                
                const output_spatial = input_spatial.clone();
                for (let c = 0; c < C; c++) {
                    const time_val = time_features_1D.get(c);
                    for (let y = 0; y < H; y++) {
                        for (let x = 0; x < W; x++) {
                            output_spatial.setCHW(c, y, x, output_spatial.getCHW(c,y,x) + time_val);
                        }
                    }
                }
                return output_spatial;
            }
            backward(dOutput_spatial) { 
                const [C, H, W] = dOutput_spatial.shape;
                const dInput_spatial = dOutput_spatial.clone(); 
                const dTime_features_1D = new Tensor([C]).fill(0);
                for (let c = 0; c < C; c++) {
                    let sum_grad_time = 0;
                    for (let y = 0; y < H; y++) {
                        for (let x = 0; x < W; x++) {
                            sum_grad_time += dOutput_spatial.getCHW(c, y, x);
                        }
                    }
                    dTime_features_1D.set(c, sum_grad_time);
                }
                return [dInput_spatial, dTime_features_1D];
            }
        }
        class UNet { /* ... (Identical structure, relies on fixed layers) ... */ 
            constructor(imageSize = 28, baseFilters = 8, timeEmbedDim = 16) {
                this.imageSize = imageSize;
                this.baseFilters = baseFilters; 
                const F = [baseFilters, baseFilters * 2, baseFilters * 4]; 

                this.time_mlp1 = new DenseLayer(1, timeEmbedDim);
                this.time_relu1 = new ReLULayer();
                this.time_mlp2 = new DenseLayer(timeEmbedDim, F[0]); 

                this.enc_conv_in = new Conv2DLayer(1, F[0]); 
                this.time_add = new TimeBroadcastAddLayer();
                this.enc_relu_in = new ReLULayer();
                
                this.enc_conv1 = new Conv2DLayer(F[0], F[0]); 
                this.enc_relu1 = new ReLULayer();
                this.pool1 = new MaxPool2DLayer(); 

                this.enc_conv2 = new Conv2DLayer(F[0], F[1]);
                this.enc_relu2 = new ReLULayer();
                this.pool2 = new MaxPool2DLayer(); 

                this.bottleneck_conv = new Conv2DLayer(F[1], F[2]);
                this.bottleneck_relu = new ReLULayer();

                this.dec_upsample1 = new UpsampleNearestLayer(); 
                this.dec_concat1 = new ConcatLayer();
                this.dec_conv1 = new Conv2DLayer(F[2] + F[1], F[1]); 
                this.dec_relu1 = new ReLULayer();

                this.dec_upsample2 = new UpsampleNearestLayer(); 
                this.dec_concat2 = new ConcatLayer();
                this.dec_conv2 = new Conv2DLayer(F[1] + F[0], F[0]); 
                this.dec_relu2 = new ReLULayer();

                this.out_conv = new Conv2DLayer(F[0], 1); 
                this.out_tanh = new TanhLayer();
            }
            predict(imageTensor, t_norm_scalar) { 
                let t_feat = new Tensor([1], new Float32Array([t_norm_scalar]));
                t_feat = this.time_mlp1.forward(t_feat);
                t_feat = this.time_relu1.forward(t_feat);
                const time_embedding_vec = this.time_mlp2.forward(t_feat);

                let h = this.enc_conv_in.forward(imageTensor);    
                h = this.time_add.forward(h, time_embedding_vec); 
                const skip1 = this.enc_relu_in.forward(h);        
                
                h = this.enc_conv1.forward(skip1);
                h = this.enc_relu1.forward(h);
                h = this.pool1.forward(h);                      
                
                h = this.enc_conv2.forward(h);
                const skip2 = this.enc_relu2.forward(h);        
                h = this.pool2.forward(skip2);                  

                h = this.bottleneck_conv.forward(h);
                h = this.bottleneck_relu.forward(h);            

                h = this.dec_upsample1.forward(h);              
                h = this.dec_concat1.forward(h, skip2);         
                h = this.dec_conv1.forward(h);
                h = this.dec_relu1.forward(h);                  

                h = this.dec_upsample2.forward(h);              
                h = this.dec_concat2.forward(h, skip1);         
                h = this.dec_conv2.forward(h);
                h = this.dec_relu2.forward(h);                  

                h = this.out_conv.forward(h);                   
                const predicted_noise_tensor = this.out_tanh.forward(h); 
                
                for(let k=0; k<predicted_noise_tensor.data.length; ++k) {
                    if (isNaN(predicted_noise_tensor.data[k])) {
                        log(`NaN found in prediction at index ${k}! t_norm: ${t_norm_scalar.toFixed(4)}`);
                        const nanTensor = new Tensor(predicted_noise_tensor.shape);
                        nanTensor.data.fill(NaN);
                        return nanTensor;
                    }
                }
                return predicted_noise_tensor;
            }
            trainStep(imageTensor, t_norm_scalar, targetNoiseTensor, learningRate) {
                const predictedNoiseTensor = this.predict(imageTensor, t_norm_scalar);
                if (isNaN(predictedNoiseTensor.data[0])) { 
                    log("Prediction was NaN, skipping train step.");
                    return NaN; 
                }

                const dErrorTensor = new Tensor(predictedNoiseTensor.shape);
                let loss = 0;
                for (let i = 0; i < predictedNoiseTensor.data.length; i++) {
                    const diff = predictedNoiseTensor.data[i] - targetNoiseTensor.data[i];
                    dErrorTensor.data[i] = 2 * diff / predictedNoiseTensor.data.length;
                    loss += diff * diff;
                }
                loss /= predictedNoiseTensor.data.length;
                if (isNaN(loss)) { log("Loss became NaN after calculation!"); return NaN; }

                let grad = this.out_tanh.backward(dErrorTensor);
                grad = this.out_conv.backward(grad, learningRate);
                grad = this.dec_relu2.backward(grad);
                grad = this.dec_conv2.backward(grad, learningRate);
                let [dGrad_up2, dGrad_skip1] = this.dec_concat2.backward(grad); 
                grad = this.dec_upsample2.backward(dGrad_up2); 
                grad = this.dec_relu1.backward(grad);
                grad = this.dec_conv1.backward(grad, learningRate);
                let [dGrad_up1, dGrad_skip2] = this.dec_concat1.backward(grad);
                grad = this.dec_upsample1.backward(dGrad_up1); 
                grad = this.bottleneck_relu.backward(grad);
                grad = this.bottleneck_conv.backward(grad, learningRate);
                grad = this.pool2.backward(grad); 
                for(let i=0; i<grad.data.length; ++i) grad.data[i] += dGrad_skip2.data[i]; 
                grad = this.enc_relu2.backward(grad); 
                grad = this.enc_conv2.backward(grad, learningRate);
                grad = this.pool1.backward(grad); 
                grad = this.enc_relu1.backward(grad);
                grad = this.enc_conv1.backward(grad, learningRate);
                for(let i=0; i<grad.data.length; ++i) grad.data[i] += dGrad_skip1.data[i]; 
                grad = this.enc_relu_in.backward(grad); 
                let [dGrad_spatial, dGrad_time_vec] = this.time_add.backward(grad); 
                grad = this.enc_conv_in.backward(dGrad_spatial, learningRate); 
                let dTime = this.time_mlp2.backward(dGrad_time_vec, learningRate);
                dTime = this.time_relu1.backward(dTime);
                this.time_mlp1.backward(dTime, learningRate); 
                return loss;
            }
        }

        // --- Diffusion Process (Identical) ---
        function precomputeDiffusionSchedule(T, beta_start, beta_end) { /* ... (Identical) ... */ 
            const betas = new Float32Array(T);
            const alphas = new Float32Array(T);
            const alphas_cumprod = new Float32Array(T);
            for (let t = 0; t < T; t++) {
                betas[t] = beta_start + (t / (T - 1)) * (beta_end - beta_start);
                alphas[t] = 1.0 - betas[t];
                alphas_cumprod[t] = (t > 0 ? alphas_cumprod[t-1] : 1.0) * alphas[t];
            }
            return {
                betas, alphas, alphas_cumprod,
                sqrt_alphas_cumprod: alphas_cumprod.map(Math.sqrt),
                sqrt_one_minus_alphas_cumprod: alphas_cumprod.map(a => Math.sqrt(1.0 - a)),
                T: T
            };
        }
        function forwardProcess(x0_flat, t_idx, dp) { /* ... (Identical) ... */
            const noise_flat = generateGaussianNoise(x0_flat.length);
            const sqrt_alpha_cumprod_t = dp.sqrt_alphas_cumprod[t_idx];
            const sqrt_one_minus_alpha_cumprod_t = dp.sqrt_one_minus_alphas_cumprod[t_idx];
            const xt_flat = new Float32Array(x0_flat.length);
            for (let i = 0; i < x0_flat.length; i++) {
                xt_flat[i] = sqrt_alpha_cumprod_t * x0_flat[i] + sqrt_one_minus_alpha_cumprod_t * noise_flat[i];
            }
            return { xt_flat, noise_flat };
        }
        function denoiseSingleStep(xt_tensor, t_idx, predicted_noise_tensor, dp) { /* ... (Identical, with NaN safety and epsilon) ... */
            const alpha_t = dp.alphas[t_idx];
            const beta_t = dp.betas[t_idx];
            const sqrt_alpha_t = Math.sqrt(alpha_t);
            const sqrt_one_minus_alpha_cumprod_t = dp.sqrt_one_minus_alphas_cumprod[t_idx];
            const x_prev_tensor = xt_tensor.clone(); 
            let z_flat = null;
            if (t_idx > 0) z_flat = generateGaussianNoise(xt_tensor.data.length);

            for (let i = 0; i < xt_tensor.data.length; i++) {
                const pred_noise_val = predicted_noise_tensor.data[i];
                if(isNaN(pred_noise_val)) { 
                    log(`NaN in predicted noise during denoiseSingleStep at t=${t_idx}`);
                    x_prev_tensor.data.fill(0); 
                    return x_prev_tensor;
                }
                const term1_numerator = xt_tensor.data[i] - (beta_t / (sqrt_one_minus_alpha_cumprod_t + 1e-8)) * pred_noise_val;
                const term1 = term1_numerator / (sqrt_alpha_t + 1e-8) ;

                let term2 = 0;
                if (t_idx > 0) {
                    const sigma_t = Math.sqrt(beta_t); 
                    term2 = sigma_t * z_flat[i];
                }
                x_prev_tensor.data[i] = term1 + term2;
            }
            return x_prev_tensor;
        }
        function reconstructX0(xt_tensor, t_idx, predicted_noise_tensor, dp) { /* ... (Identical, with NaN safety and epsilon) ... */
            const sqrt_alpha_cumprod_t = dp.sqrt_alphas_cumprod[t_idx];
            const sqrt_one_minus_alpha_cumprod_t = dp.sqrt_one_minus_alphas_cumprod[t_idx];
            const x0_reconstructed_tensor = xt_tensor.clone();
            for (let i = 0; i < xt_tensor.data.length; i++) {
                 const pred_noise_val = predicted_noise_tensor.data[i];
                 if(isNaN(pred_noise_val)) { 
                    x0_reconstructed_tensor.data.fill(0);
                    return x0_reconstructed_tensor;
                 }
                x0_reconstructed_tensor.data[i] = (xt_tensor.data[i] - sqrt_one_minus_alpha_cumprod_t * pred_noise_val) / (sqrt_alpha_cumprod_t + 1e-8);
            }
            return x0_reconstructed_tensor;
        }

        // --- Training & Generation (Modified for Pause/Resume) ---
        function updateButtonStates() {
            if (isTrainingActive) {
                trainButton.textContent = 'Restart Training'; // Or 'Stop Training' if you want full stop
                trainButton.disabled = false; // Allow restart even if paused
                pauseResumeButton.disabled = false;
                pauseResumeButton.textContent = isTrainingPaused ? 'Resume Training' : 'Pause Training';
                generateButton.disabled = !isTrainingPaused; // Enable only if paused
                testDenoiseButton.disabled = !isTrainingPaused; // Enable only if paused
            } else { // Not active (before start or after completion/full stop)
                trainButton.textContent = 'Start Training';
                trainButton.disabled = false;
                pauseResumeButton.disabled = true;
                pauseResumeButton.textContent = 'Pause Training';
                generateButton.disabled = (model === null); // Enable if model exists
                testDenoiseButton.disabled = (model === null || mnistData.length === 0); // Enable if model and data exist
            }
        }
        
        async function trainingLoop() {
            isTrainingActive = true;
            updateButtonStates();

            const T_diffusion = diffusionParams.T; // Use precomputed T

            log(`Starting/Resuming U-Net training (model: BF=2, TE=4) from Epoch ${currentEpoch + 1}, Image ${currentImageIndex + 1}...`);
            log(`LR=${learningRateVal.toExponential(1)}, Diffusion_T=${T_diffusion}`);

            let epochLoss = 0;
            let epochValidStepCount = 0;

            for (; currentEpoch < totalEpochsGoal; currentEpoch++) {
                epochLoss = 0; // Reset for current epoch if resuming mid-epoch
                epochValidStepCount = 0;
                const startTimeEpoch = Date.now();

                for (; currentImageIndex < totalImagesPerEpoch; currentImageIndex++) {
                    if (isTrainingPaused) {
                        log(`Training paused at Epoch ${currentEpoch + 1}, Image ${currentImageIndex + 1}.`);
                        updateButtonStates();
                        await new Promise(resolve => { resolvePause = resolve; }); // Wait here
                        if (!isTrainingActive) { // If training was stopped/restarted while paused
                             log("Training was stopped/restarted while paused. Exiting loop.");
                             return; 
                        }
                        log(`Resuming training from Epoch ${currentEpoch + 1}, Image ${currentImageIndex + 1}...`);
                        updateButtonStates();
                    }
                    if (!isTrainingActive) return; // For full stop/restart

                    const x0_flat = mnistData[currentImageIndex]; 
                    const x0_tensor = Tensor.fromFlat(x0_flat, IMAGE_SIZE, IMAGE_SIZE, 1); 
                    
                    const t_idx = Math.floor(Math.random() * T_diffusion); 
                    const t_norm = (t_idx + 1) / T_diffusion; 

                    const { xt_flat, noise_flat: true_noise_flat } = forwardProcess(x0_flat, t_idx, diffusionParams);
                    const xt_tensor = Tensor.fromFlat(xt_flat, IMAGE_SIZE, IMAGE_SIZE, 1);
                    const true_noise_tensor = Tensor.fromFlat(true_noise_flat, IMAGE_SIZE, IMAGE_SIZE, 1);
                    
                    const loss = model.trainStep(xt_tensor, t_norm, true_noise_tensor, learningRateVal);
                    
                    if (!isNaN(loss)) {
                        epochLoss += loss;
                        totalLossSinceStart += loss;
                        epochValidStepCount++;
                        validLossCount++;
                    }

                    if ((currentImageIndex + 1) % Math.max(1, Math.floor(totalImagesPerEpoch / 5)) === 0 || currentImageIndex === totalImagesPerEpoch -1 || currentImageIndex === 0) {
                        const avgEpochLossStr = epochValidStepCount > 0 ? (epochLoss / epochValidStepCount).toFixed(5) : "NaN";
                        const avgTotalLossStr = validLossCount > 0 ? (totalLossSinceStart / validLossCount).toFixed(5) : "NaN";
                        log(`Epoch ${currentEpoch+1}/${totalEpochsGoal}, Img ${currentImageIndex+1}/${totalImagesPerEpoch}, Loss: ${isNaN(loss) ? "NaN" : loss.toFixed(5)}, AvgEpochL: ${avgEpochLossStr}, AvgTotalL: ${avgTotalLossStr}`);
                        
                        if (!isTrainingPaused) { // Don't draw if we are about to pause
                            drawToCanvas('originalCanvas', x0_tensor.toFlat());
                            drawToCanvas('noisyCanvas', xt_tensor.toFlat());
                            const predicted_noise_tensor_viz = model.predict(xt_tensor, t_norm);
                            if (!isNaN(predicted_noise_tensor_viz.data[0])) {
                               drawToCanvas('predictedNoiseCanvas', predicted_noise_tensor_viz.toFlat());
                               const reconstructed_x0_viz = reconstructX0(xt_tensor, t_idx, predicted_noise_tensor_viz, diffusionParams);
                               drawToCanvas('denoisedStepCanvas', reconstructed_x0_viz.toFlat());
                            } else {
                               log("Visualization skipped due to NaN prediction.");
                            }
                        }
                        await new Promise(resolve => setTimeout(resolve, 5)); // Shorter yield
                    }
                } // End image loop
                const epochTime = (Date.now() - startTimeEpoch) / 1000;
                const finalEpochAvgLossStr = epochValidStepCount > 0 ? (epochLoss / epochValidStepCount).toFixed(5) : "NaN";
                log(`Epoch ${currentEpoch+1} finished. Avg Epoch Loss: ${finalEpochAvgLossStr}. Valid steps: ${epochValidStepCount}/${totalImagesPerEpoch}. Time: ${epochTime.toFixed(2)}s`);
                currentImageIndex = 0; // Reset for next epoch
            } // End epoch loop

            log(`Training complete! Total valid steps: ${validLossCount}/${totalEpochsGoal*totalImagesPerEpoch}`);
            isTrainingActive = false;
            isTrainingPaused = false; // Ensure pause is off
            currentEpoch = 0; // Reset for next "Start Training"
            currentImageIndex = 0;
            updateButtonStates();
        }

        async function startOrRestartTraining() {
            if (isTrainingActive && !isTrainingPaused) { // If running, treat as restart
                log("Restarting training...");
                isTrainingActive = false; // Signal current loop to stop
                if (resolvePause) resolvePause(); // Unblock if it was paused then restart clicked
                await new Promise(r => setTimeout(r, 50)); // Give it a moment to exit
            } else if (isTrainingPaused) {
                 log("Restarting training from paused state...");
                 isTrainingActive = false;
                 isTrainingPaused = false;
                 if (resolvePause) resolvePause();
                 await new Promise(r => setTimeout(r, 50));
            }


            // Initialize / Reset state for a new training run
            isTrainingActive = true;
            isTrainingPaused = false;
            currentEpoch = 0;
            currentImageIndex = 0;
            validLossCount = 0;
            totalLossSinceStart = 0;

            const numTrainImagesUI = parseInt(document.getElementById('numTrainImages').value);
            totalEpochsGoal = parseInt(document.getElementById('epochs').value);
            learningRateVal = parseFloat(document.getElementById('learningRate').value); // Store parsed LR
            const T_diffusion_ui = parseInt(document.getElementById('timestepsT').value);
            const betaStart = parseFloat(document.getElementById('betaStart').value);
            const betaEnd = parseFloat(document.getElementById('betaEnd').value);

            // Load data if not already loaded or if numImages changed (simplified: always reload for now)
            // A more complex check could compare numTrainImagesUI with mnistData.length
            await loadAndParseMNIST(numTrainImagesUI);
            if (mnistData.length === 0) { 
                log("MNIST data not loaded. Cannot start training."); 
                isTrainingActive = false;
                updateButtonStates();
                return; 
            }
            totalImagesPerEpoch = mnistData.length;
            
            // Re-initialize model and diffusion params if it's a true restart or first time
            // Or if key params changed. For simplicity, re-init model if not existing.
            if (!model) { // Or if params changed significantly
                diffusionParams = precomputeDiffusionSchedule(T_diffusion_ui, betaStart, betaEnd);
                model = new UNet(IMAGE_SIZE, 2, 4); // Small U-Net
            } else { // If model exists, ensure diffusionParams are current
                 diffusionParams = precomputeDiffusionSchedule(T_diffusion_ui, betaStart, betaEnd);
            }


            trainingLoop(); // Start the async loop
        }

        function togglePauseResume() {
            if (!isTrainingActive) return;

            isTrainingPaused = !isTrainingPaused;
            if (!isTrainingPaused && resolvePause) { // Resuming
                resolvePause();
                resolvePause = null; // Consume it
            }
            updateButtonStates();
            if(isTrainingPaused) log("Training Paused. Inference buttons enabled.");
            else log("Training Resumed.");
        }


        async function generateSample() { /* ... (Identical, ensure clamping and NaN checks) ... */
            if (!model || !diffusionParams.T) { log("Model not trained."); return; }
            if (isTrainingActive && !isTrainingPaused) { log("Pause training before generating."); return; }

            generateButton.disabled = true; testDenoiseButton.disabled = true; // Disable during generation
            log("Generating sample from pure noise (U-Net)...");

            let xt_flat = generateGaussianNoise(IMAGE_SIZE * IMAGE_SIZE);
            let xt_tensor = Tensor.fromFlat(xt_flat, IMAGE_SIZE, IMAGE_SIZE, 1);
            drawToCanvas('generatedCanvas', xt_tensor.toFlat());

            const Tval = diffusionParams.T; 
            for (let t_idx = Tval - 1; t_idx >= 0; t_idx--) {
                const t_norm = (t_idx + 1) / Tval;
                const predicted_noise_tensor = model.predict(xt_tensor, t_norm);

                if(isNaN(predicted_noise_tensor.data[0])) {
                    log(`NaN prediction during generation at t=${t_idx}. Stopping.`);
                    drawToCanvas('generatedCanvas', xt_tensor.toFlat()); 
                    updateButtonStates(); // Re-enable buttons based on current training state
                    return;
                }

                xt_tensor = denoiseSingleStep(xt_tensor, t_idx, predicted_noise_tensor, diffusionParams);
                for(let i=0; i < xt_tensor.data.length; i++) { 
                     xt_tensor.data[i] = Math.max(-1, Math.min(1, xt_tensor.data[i]));
                }

                if (t_idx % Math.max(1,Math.floor(Tval/10)) === 0 || t_idx === 0) {
                    log(`Denoising step t=${t_idx+1}/${Tval}`);
                    drawToCanvas('generatedCanvas', xt_tensor.toFlat());
                    await new Promise(resolve => setTimeout(resolve, 10));
                }
            }
            log("Sample generation finished.");
            drawToCanvas('generatedCanvas', xt_tensor.toFlat());
            updateButtonStates();
        }
        
        async function testDenoisingRandomImage() { /* ... (Identical, ensure clamping and NaN checks and button updates) ... */
            if (!model || !diffusionParams.T || mnistData.length === 0) { log("Model/data not ready."); return; }
            if (isTrainingActive && !isTrainingPaused) { log("Pause training before testing."); return; }

            generateButton.disabled = true; testDenoiseButton.disabled = true; // Disable during test
            log("Testing U-Net denoising on a random MNIST image...");

            const x0_flat = mnistData[Math.floor(Math.random() * mnistData.length)];
            const x0_tensor = Tensor.fromFlat(x0_flat, IMAGE_SIZE, IMAGE_SIZE, 1);
            drawToCanvas('originalCanvas', x0_tensor.toFlat());

            const Tval = diffusionParams.T;
            const t_test_idx = Math.floor(Tval / 2);
            log(`Noising image to t=${t_test_idx + 1}/${Tval}`);
            const { xt_flat: xt_test_flat } = forwardProcess(x0_flat, t_test_idx, diffusionParams);
            const xt_test_tensor = Tensor.fromFlat(xt_test_flat, IMAGE_SIZE, IMAGE_SIZE, 1);
            drawToCanvas('noisyCanvas', xt_test_tensor.toFlat());
            
            const t_norm_test = (t_test_idx + 1) / Tval;
            const predicted_noise_test = model.predict(xt_test_tensor, t_norm_test);
             if(isNaN(predicted_noise_test.data[0])) {
                log("NaN prediction during test denoise. Aborting full reverse.");
                updateButtonStates(); return;
            }
            drawToCanvas('predictedNoiseCanvas', predicted_noise_test.toFlat());

            const reconstructed_x0_test = reconstructX0(xt_test_tensor, t_test_idx, predicted_noise_test, diffusionParams);
            drawToCanvas('denoisedStepCanvas', reconstructed_x0_test.toFlat());
            log("Denoising test finished. Check 'Denoised (from x_t)' canvas.");
            
            log("Now attempting full reverse process from this noised image (xt_test)...");
            let current_x_tensor = xt_test_tensor.clone();
            drawToCanvas('generatedCanvas', current_x_tensor.toFlat()); 
            
            for (let t_idx_loop = t_test_idx; t_idx_loop >= 0; t_idx_loop--) {
                const t_norm_loop = (t_idx_loop + 1) / Tval;
                const pred_noise_loop = model.predict(current_x_tensor, t_norm_loop);
                 if(isNaN(pred_noise_loop.data[0])) {
                    log(`NaN prediction during full reverse at t=${t_idx_loop}. Stopping.`);
                    break; 
                }
                current_x_tensor = denoiseSingleStep(current_x_tensor, t_idx_loop, pred_noise_loop, diffusionParams);
                for(let i=0; i < current_x_tensor.data.length; i++) current_x_tensor.data[i] = Math.max(-1, Math.min(1, current_x_tensor.data[i]));

                if (t_idx_loop % Math.max(1,Math.floor((t_test_idx+1)/5)) === 0 || t_idx_loop === 0) {
                    log(`Denoising step t=${t_idx_loop+1}/${Tval}`);
                    drawToCanvas('generatedCanvas', current_x_tensor.toFlat());
                    await new Promise(resolve => setTimeout(resolve, 10));
                }
            }
            log("Full reverse process from noised image finished.");
            drawToCanvas('generatedCanvas', current_x_tensor.toFlat());
            updateButtonStates();
        }

        // --- Event Listeners & Init ---
        trainButton.addEventListener('click', startOrRestartTraining);
        pauseResumeButton.addEventListener('click', togglePauseResume);
        generateButton.addEventListener('click', generateSample);
        testDenoiseButton.addEventListener('click', testDenoisingRandomImage);

        log("U-Net Demo with Pause/Resume ready.");
        updateButtonStates(); // Initial button states

    </script>
</body>
</html>
