<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>今日学术速递</title>
    <style>
        body {
            font-family: 'Inter', 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
        }

        h1 {
            text-align: center;
            color: #333;
        }

        p {
            margin-bottom: 15px;
            color: #555;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 20px;
        }

        th, td {
            padding: 12px 15px;
            border: 1px solid #ddd;
            text-align: left;
        }

        th {
            background-color: #f2f2f2;
            font-weight: bold;
        }

        tr:nth-child(even) {
            background-color: #f9f9f9;
        }

        a {
            color: #007bff;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        .paper-row {
            position: relative;
        }

        .tooltip-text {
            visibility: hidden;
            opacity: 0;
            position: absolute;
            background-color: #f9f9f9;
            border: 1px solid #ccc;
            padding: 10px;
            z-index: 1;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
            transition: opacity 0.2s ease-in-out;
            max-width: 600px;
            word-wrap: break-word;
        }

        .paper-row:hover .tooltip-text {
            visibility: visible;
            opacity: 1;
        }
    </style>
</head>
<body>
    <h1>今日学术速递</h1>
    <p>以下是根据周舒畅博士学术兴趣画像推荐的最新学术论文摘要，希望能为您提供参考。</p>

    <h2>综合推荐 Top 10</h2>
    <table>
        <thead>
            <tr>
                <th>论文标题</th>
            </tr>
        </thead>
        <tbody>
            <tr class="paper-row">
                <td><a href="https://arxiv.org/abs/2507.06221" target="_blank">对齐的文本评分规则</a> <span class="tooltip-text"><b>作者:</b> Yuxuan Lu, Yifan Wu, Jason Hartline, Michael J. Curry<br><b>中文摘要:</b> 评分规则通过根据真实状态对预测进行评分来从战略代理中提取概率预测。如果从代理的角度来看，报告真实信念可以最大化预期分数，则该评分规则是适当的。随着语言模型的发展，Wu 和 Hartline (2024) 提出了一种将文本信息提取简化为数值（即概率）信息提取问题的方案，从而实现了文本提取的可靠适当性。然而，并非所有适当的评分规则都与人类对文本的偏好良好对齐。本文通过优化并最小化适当评分规则与参考分数（例如，人类分数）之间的均方误差，为文本设计了对齐评分规则 (ASR)。我们的实验表明，在保持适当性的同时，我们的 ASR 在与人类偏好对齐方面优于以前的方法。<br><b>English Abstract:</b> Scoring rules elicit probabilistic predictions from a strategic agent by scoring the prediction against a ground truth state. A scoring rule is proper if, from the agent's perspective, reporting the true belief maximizes the expected score. With the development of language models, Wu and Hartline (2024) proposes a reduction from textual information elicitation to the numerical (i.e. probabilistic) information elicitation problem, which achieves provable properness for textual elicitation. However, not all proper scoring rules are well aligned with human preference over text. Our paper designs the Aligned Scoring rule (ASR) for text by optimizing and minimizing the mean squared error between a proper scoring rule and a reference score (e.g. human score). Our experiments show that our ASR outperforms previous methods in aligning with human preference while maintaining properness.</a> <a href="https://arxiv.org/pdf/2507.06221" target="_blank">[PDF]</a></td>
            </tr>
            <tr class="paper-row">
                <td><a href="https://arxiv.org/abs/2507.06187" target="_blank">Delta学习假设：弱数据上的偏好调整可以带来显著收益</a> <span class="tooltip-text"><b>作者:</b> Scott Geng, Hamish Ivison, Chun-Liang Li, Maarten Sap, Jerry Li, Ranjay Krishna, Pang Wei Koh<br><b>中文摘要:</b> 语言模型的改进通常依赖于提高训练数据的质量，但在缺乏强监督的情况下，这可能会受到限制。在这项工作中，我们表明，由单独的弱数据点组成的配对偏好数据可以实现超越每个单独数据点强度的收益。我们提出了Delta学习假设来解释这一现象，认为点之间的相对质量差异足以通过偏好调整来驱动学习——即使对弱数据进行监督微调反而会损害性能。我们在受控实验和大规模实验中验证了我们的假设，其中我们使用一个小型3B模型的响应与一个更小的1.5B模型的输出配对来生成偏好数据，从而创建一个有意义的Delta，对8B模型进行后训练。令人惊讶的是，在标准的11个基准测试套件（MATH、MMLU等）上，我们的简单方法与从同一基础模型调整而来的最先进的开源模型Tulu 3的性能相匹配，同时依赖于更强的监督者（例如，GPT-4o）。因此，Delta学习能够实现更简单、更便宜的开源后训练方法。<br><b>English Abstract:</b> Improvements in language models are often driven by improving the quality of the data we train them on, which can be limiting when strong supervision is scarce. In this work, we show that paired preference data consisting of individually weak data points can enable gains beyond the strength of each individual data point. We formulate the delta learning hypothesis to explain this phenomenon, positing that the relative quality delta between points suffices to drive learning via preference tuning--even when supervised finetuning on the weak data hurts. We validate our hypothesis in controlled experiments and at scale, where we post-train 8B models on preference data generated by pairing a small 3B model's responses with outputs from an even smaller 1.5B model to create a meaningful delta. Strikingly, on a standard 11-benchmark evaluation suite (MATH, MMLU, etc.), our simple recipe matches the performance of Tulu 3, a state-of-the-art open model tuned from the same base model while relying on much stronger supervisors (e.g., GPT-4o). Thus, delta learning enables simpler and cheaper open recipes for state-of-the-art post-training.</span></td>
            </tr>
            <tr class="paper-row">
                <td><a href="https://arxiv.org/abs/2507.06134" target="_blank">OpenAgentSafety：评估真实世界AI代理安全性的全面框架</a> <span class="tooltip-text"><b>作者:</b> Sanidhya Vijayvargiya, Aditya Bharat Soni, Xuhui Zhou, Zora Zhiruo Wang, Nouha Dziri, Graham Neubig, Maarten Sap<br><b>中文摘要:</b> 近期，能够解决复杂日常任务（从日程安排到客户服务）的AI代理取得了显著进展，并已部署到真实环境中，但其潜在的不安全行为需要严格评估。虽然先前的基准测试试图评估代理的安全性，但大多数都存在不足，例如依赖模拟环境、狭窄的任务领域或不真实的工具抽象。我们引入OpenAgentSafety，这是一个用于评估代理在八个关键风险类别中的行为的全面且模块化框架。与先前的工作不同，我们的框架评估与真实工具交互的代理，包括网络浏览器、代码执行环境、文件系统、bash shell和消息平台；并且支持超过350个多轮、多用户任务，涵盖良性和对抗性用户意图。OpenAgentSafety的设计具有可扩展性，允许研究人员以最小的努力添加工具、任务、网站和对抗策略。它结合了基于规则的分析和LLM作为评估者，以检测明显和微妙的不安全行为。对五种著名LLM在代理场景中的实证分析表明，在51.2%的安全漏洞任务中，Claude-Sonnet-3.7存在不安全行为，而o3-mini则高达72.7%，这凸显了关键的安全漏洞，以及在实际部署之前需要更强的保障措施。<br><b>English Abstract:</b> Recent advances in AI agents capable of solving complex, everyday tasks, from scheduling to customer service, have enabled deployment in real-world settings, but their possibilities for unsafe behavior demands rigorous evaluation. While prior benchmarks have attempted to assess agent safety, most fall short by relying on simulated environments, narrow task domains, or unrealistic tool abstractions. We introduce OpenAgentSafety, a comprehensive and modular framework for evaluating agent behavior across eight critical risk categories. Unlike prior work, our framework evaluates agents that interact with real tools, including web browsers, code execution environments, file systems, bash shells, and messaging platforms; and supports over 350 multi-turn, multi-user tasks spanning both benign and adversarial user intents. OpenAgentSafety is designed for extensibility, allowing researchers to add tools, tasks, websites, and adversarial strategies with minimal effort. It combines rule-based analysis with LLM-as-judge assessments to detect both overt and subtle unsafe behaviors. Empirical analysis of five prominent LLMs in agentic scenarios reveals unsafe behavior in 51.2% of safety-vulnerable tasks with Claude-Sonnet-3.7, to 72.7% with o3-mini, highlighting critical safety vulnerabilities and the need for stronger safeguards before real-world deployment.</span></td>
            </tr>
            <tr class="paper-row">
                <td><a href="https://arxiv.org/abs/2507.06057" target="_blank">FEVO：大型语言模型的金融知识扩展与推理演化</a> <span class="tooltip-text"><b>作者:</b> Bo Pang, Yalu Ouyang, Hangfei Xu, Ziqi Jia, Panpan Li, Shengzhao Wen, Lu Wang, Shiyong Li, Yanpeng Wang<br><b>中文摘要:</b> 大型语言模型（LLM）推理能力的进步已显著提升了其在数学和编程等领域的性能。然而，将这些进步应用于金融领域的研究仍然有限，而金融领域需要大量的特定领域知识才能完成任务。为了弥合这一差距，我们引入了FEVO（金融演化），一个多阶段增强框架，旨在提升LLM在金融领域的性能。FEVO通过持续预训练（CPT）扩展金融领域知识、监督微调（SFT）灌输结构化、精细的推理模式以及强化学习（RL）进一步整合扩展的金融领域知识与学习到的结构化推理，系统地提升LLM性能。为了确保有效且高效的训练，我们利用前沿推理模型和基于规则的过滤来策划FEVO-Train，一个专门为不同后训练阶段设计的、高质量数据集。利用我们的框架，我们从Qwen2.5-32B训练了FEVO系列模型——C32B、S32B、R32B，并在七个基准测试中评估了它们的金融和通用能力，结果表明FEVO-R32B在五个金融基准测试中实现了最先进的性能，优于更大的模型以及专业模型。更重要的是，FEVO-R32B比FEVO-R32B-0（仅使用RL从Qwen2.5-32B-Instruct训练）表现出明显更好的性能，从而验证了金融领域知识扩展和结构化、逻辑推理提炼的有效性。<br><b>English Abstract:</b> Advancements in reasoning for large language models (LLMs) have lead to significant performance improvements for LLMs in various fields such as mathematics and programming. However, research applying these advances to the financial domain, where considerable domain-specific knowledge is necessary to complete tasks, remains limited. To address this gap, we introduce FEVO (Financial Evolution), a multi-stage enhancement framework developed to enhance LLM performance in the financial domain. FEVO systemically enhances LLM performance by using continued pre-training (CPT) to expand financial domain knowledge, supervised fine-tuning (SFT) to instill structured, elaborate reasoning patterns, and reinforcement learning (RL) to further integrate the expanded financial domain knowledge with the learned structured reasoning. To ensure effective and efficient training, we leverage frontier reasoning models and rule-based filtering to curate FEVO-Train, high-quality datasets specifically designed for the different post-training phases. Using our framework, we train the FEVO series of models -- C32B, S32B, R32B -- from Qwen2.5-32B and evaluate them on seven benchmarks to assess financial and general capabilities, with results showing that FEVO-R32B achieves state-of-the-art performance on five financial benchmarks against much larger models as well as specialist models. More significantly, FEVO-R32B demonstrates markedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct using only RL), thus validating the effectiveness of financial domain knowledge expansion and structured, logical reasoning distillation</span></td>
            </tr>
            <tr class="paper-row">
                <td><a href="https://arxiv.org/abs/2507.06013" target="_blank">CogniSQL-R1-Zero：轻量级强化推理，用于高效SQL生成</a> <span class="tooltip-text"><b>作者:</b> Kushal Gajjar, Harshit Sikchi, Arpit Singh Gautam, Marc Hammons, Saurabh Jha<br><b>中文摘要:</b> 将自然语言翻译成SQL（文本到SQL）仍然是语言理解和结构化数据访问交叉领域的核心挑战。尽管大型语言模型（LLM）提高了流畅性，但生成正确的、可执行的SQL，尤其对于复杂的查询，仍然具有挑战性。我们引入CogniSQL-R1-Zero，这是一种强化学习（RL）框架和模型，它使用基于执行正确性和格式标签合规性的轻量级奖励信号来生成准确的SQL。通过避免中间监督、混合管道和复杂的奖励塑造，我们的方法鼓励了稳定的学习，并与最终任务目标（生成可执行程序）建立了更强的对齐。CogniSQL-R1-Zero在Text2SQL基准测试BIRD bench上实现了最先进的执行准确率，优于先前的监督学习和指令微调基线，包括SFT CodeS-7B、DeepSeek-Coder 236B和Mistral 123B——尽管它是在一个显著较小的7B骨干网络上训练的。这一结果强调了我们在仅使用四块NVIDIA A100 GPU（每块40 GB VRAM）训练时，基于RL的方法的可扩展性和效率。为了支持对高效且可解释的文本到SQL建模的进一步研究，我们发布了两个策划的数据集：（i）一个包含5,024个具有不同上下文长度的推理轨迹的集合，以及（ii）一个包含36,356个弱监督查询的正样本语料库，每个查询都附带六条语义上多样的推理路径。总之，这些贡献推动了可扩展的、与执行对齐的文本到SQL生成。<br><b>English Abstract:</b> Translating natural language into SQL (Text-to-SQL) remains a core challenge at the intersection of language understanding and structured data access. Although large language models (LLMs) have improved fluency, generating correct and executable SQL, especially for complex queries, continues to be challenging. We introduce CogniSQL-R1-Zero, a reinforcement learning (RL) framework and model that produces accurate SQL using a lightweight reward signal based on execution correctness and format-tag compliance. By avoiding intermediate supervision, hybrid pipelines and complex reward shaping, our method encourages stable learning and stronger alignment with the ultimate task objective-producing executable programs. CogniSQL-R1-Zero achieves state-of-the-art execution accuracy on Text2SQL benchmark; BIRD bench, outperforming prior supervised and instruction-tuned baselines including SFT CodeS-7B, DeepSeek-Coder 236B, and Mistral 123B-despite being trained on a significantly smaller 7B backbone. This result underscores the scalability and efficiency of our RL-based approach when trained on just four NVIDIA A100 GPUs (40 GB VRAM each). To support further research in efficient and interpretable Text-to-SQL modeling, we release two curated datasets: (i) a collection of 5,024 reasoning traces with varying context lengths, and (ii) a positive-sampled corpus of 36,356 corpus of weakly supervised queries, each annotated with six semantically diverse reasoning paths. Together, these contributions advance scalable, execution-aligned Text-to-SQL generation.</span></td>
            </tr>
            <tr class="paper-row">
                <td><a href="https://arxiv.org/abs/2507.06029" target="_blank">特征引导的邻居选择用于模型预测的非专家评估</a> <span class="tooltip-text"><b>作者:</b> Courtney Ford, Mark T. Keane<br><b>中文摘要:</b> 可解释人工智能 (XAI) 方法通常难以为缺乏领域专业知识的用户生成清晰、可解释的输出。我们引入了特征引导的邻居选择 (FGNS)，这是一种事后方法，通过使用局部和全局特征重要性选择具有代表性的类别示例来增强可解释性。在一项用户研究 (N = 98) 中，评估 Kannada 脚本分类，FGNS 显著提高了非专家识别模型错误的能力，同时保持了与正确预测的适当一致性。参与者做出了更快、更准确的决策，与获得传统 k-NN 解释的参与者相比。定量分析表明，FGNS 选择的邻居更好地反映了类别特征，而不仅仅是最小化特征空间距离，从而实现更一致的选择和围绕类别原型的更紧密聚类。这些结果支持 FGNS 作为迈向更符合人类评估模型的一步，尽管还需要进一步的工作来弥合解释质量和感知信任之间的差距。<br><b>English Abstract:</b> Explainable AI (XAI) methods often struggle to generate clear, interpretable outputs for users without domain expertise. We introduce Feature-Guided Neighbor Selection (FGNS), a post hoc method that enhances interpretability by selecting class-representative examples using both local and global feature importance. In a user study (N = 98) evaluating Kannada script classifications, FGNS significantly improved non-experts' ability to identify model errors while maintaining appropriate agreement with correct predictions. Participants made faster and more accurate decisions compared to those given traditional k-NN explanations. Quantitative analysis shows that FGNS selects neighbors that better reflect class characteristics rather than merely minimizing feature-space distance, leading to more consistent selection and tighter clustering around class prototypes. These results support FGNS as a step toward more human-aligned model assessment, although further work is needed to address the gap between explanation quality and perceived trust.</span></td>
            </tr>
        </tbody>
    </table>

    <h2>理论创新 Top 5</h2>
    <table>
        <thead>
            <tr>
                <th>论文标题</th>
            </tr>
        </thead>
        <tbody>
            <tr class="paper-row">
                <td><a href="https://arxiv.org/abs/2507.06221" target="_blank">对齐的文本评分规则</a> <span class="tooltip-text"><b>作者:</b> Yuxuan Lu, Yifan Wu, Jason Hartline, Michael J. Curry<br><b>中文摘要:</b> 评分规则通过根据真实状态对预测进行评分来从战略代理中提取概率预测。如果从代理的角度来看，报告真实信念可以最大化预期分数，则该评分规则是适当的。随着语言模型的发展，Wu 和 Hartline (2024) 提出了一种将文本信息提取简化为数值（即概率）信息提取问题的方案，从而实现了文本提取的可靠适当性。然而，并非所有适当的评分规则都与人类对文本的偏好良好对齐。本文通过优化并最小化适当评分规则与参考分数（例如，人类分数）之间的均方误差，为文本设计了对齐评分规则 (ASR)。我们的实验表明，在保持适当性的同时，我们的 ASR 在与人类偏好对齐方面优于以前的方法。<br><b>English Abstract:</b> Scoring rules elicit probabilistic predictions from a strategic agent by scoring the prediction against a ground truth state. A scoring rule is proper if, from the agent's perspective, reporting the true belief maximizes the expected score. With the development of language models, Wu and Hartline (2024) proposes a reduction from textual information elicitation to the numerical (i.e. probabilistic) information elicitation problem, which achieves provable properness for textual elicitation. However, not all proper scoring rules are well aligned with human preference over text. Our paper designs the Aligned Scoring rule (ASR) for text by optimizing and minimizing the mean squared error between a proper scoring rule and a reference score (e.g. human score). Our experiments show that our ASR outperforms previous methods in aligning with human preference while maintaining properness.</span></td>
            </tr>
            <tr class="paper-row">
                <td><a href="https://arxiv.org/abs/2507.06029" target="_blank">特征引导的邻居选择用于模型预测的非专家评估</a> <span class="tooltip-text"><b>作者:</b> Courtney Ford, Mark T. Keane<br><b>中文摘要:</b> 可解释人工智能 (XAI) 方法通常难以为缺乏领域专业知识的用户生成清晰、可解释的输出。我们引入了特征引导的邻居选择 (FGNS)，这是一种事后方法，通过使用局部和全局特征重要性选择具有代表性的类别示例来增强可解释性。在一项用户研究 (N = 98) 中，评估 Kannada 脚本分类，FGNS 显著提高了非专家识别模型错误的能力，同时保持了与正确预测的适当一致性。参与者做出了更快、更准确的决策，与获得传统 k-NN 解释的参与者相比。定量分析表明，FGNS 选择的邻居更好地反映了类别特征，而不仅仅是最小化特征空间距离，从而实现更一致的选择和围绕类别原型的更紧密聚类。这些结果支持 FGNS 作为迈向更符合人类评估模型的一步，尽管还需要进一步的工作来弥合解释质量和感知信任之间的差距。<br><b>English Abstract:</b> Explainable AI (XAI) methods often struggle to generate clear, interpretable outputs for users without domain expertise. We introduce Feature-Guided Neighbor Selection (FGNS), a post hoc method that enhances interpretability by selecting class-representative examples using both local and global feature importance. In a user study (N = 98) evaluating Kannada script classifications, FGNS significantly improved non-experts' ability to identify model errors while maintaining appropriate agreement with correct predictions. Participants made faster and more accurate decisions compared to those given traditional k-NN explanations. Quantitative analysis shows that FGNS selects neighbors that better reflect class characteristics rather than merely minimizing feature-space distance, leading to more consistent selection and tighter clustering around class prototypes. These results support FGNS as a step toward more human-aligned model assessment, although further work is needed to address the gap between explanation quality and perceived trust.</span></td>
            </tr>
        </tbody>
    </table>

    <h2>方法论/应用 Top 5</h2>
    <table>
        <thead>
            <tr>
                <th>论文标题</th>
            </tr>
        </thead>
        <tbody>
            <tr class="paper-row">
                <td><a href="https://arxiv.org/abs/2507.06187" target="_blank">Delta学习假设：弱数据上的偏好调整可以带来显著收益</a> <span class="tooltip-text"><b>作者:</b> Scott Geng, Hamish Ivison, Chun-Liang Li, Maarten Sap, Jerry Li, Ranjay Krishna, Pang Wei Koh<br><b>中文摘要:</b> 语言模型的改进通常依赖于提高训练数据的质量，但在缺乏强监督的情况下，这可能会受到限制。在这项工作中，我们表明，由单独的弱数据点组成的配对偏好数据可以实现超越每个单独数据点强度的收益。我们提出了Delta学习假设来解释这一现象，认为点之间的相对质量差异足以通过偏好调整来驱动学习——即使对弱数据进行监督微调反而会损害性能。我们在受控实验和大规模实验中验证了我们的假设，其中我们使用一个小型3B模型的响应与一个更小的1.5B模型的输出配对来生成偏好数据，从而创建一个有意义的Delta，对8B模型进行后训练。令人惊讶的是，在标准的11个基准测试套件（MATH、MMLU等）上，我们的简单方法与从同一基础模型调整而来的最先进的开源模型Tulu 3的性能相匹配，同时依赖于更强的监督者（例如，GPT-4o）。因此，Delta学习能够实现更简单、更便宜的开源后训练方法。<br><b>English Abstract:</b> Improvements in language models are often driven by improving the quality of the data we train them on, which can be limiting when strong supervision is scarce. In this work, we show that paired preference data consisting of individually weak data points can enable gains beyond the strength of each individual data point. We formulate the delta learning hypothesis to explain this phenomenon, positing that the relative quality delta between points suffices to drive learning via preference tuning--even when supervised finetuning on the weak data hurts. We validate our hypothesis in controlled experiments and at scale, where we post-train 8B models on preference data generated by pairing a small 3B model's responses with outputs from an even smaller 1.5B model to create a meaningful delta. Strikingly, on a standard 11-benchmark evaluation suite (MATH, MMLU, etc.), our simple recipe matches the performance of Tulu 3, a state-of-the-art open model tuned from the same base model while relying on much stronger supervisors (e.g., GPT-4o). Thus, delta learning enables simpler and cheaper open recipes for state-of-the-art post-training.</span></td>
            </tr>
            <tr class="paper-row">
                <td><a href="https://arxiv.org/abs/2507.06134" target="_blank">OpenAgentSafety：评估真实世界AI代理安全性的全面框架</a> <span class="tooltip-text"><b>作者:</b> Sanidhya Vijayvargiya, Aditya Bharat Soni, Xuhui Zhou, Zora Zhiruo Wang, Nouha Dziri, Graham Neubig, Maarten Sap<br><b>中文摘要:</b> 近期，能够解决复杂日常任务（从日程安排到客户服务）的AI代理取得了显著进展，并已部署到真实环境中，但其潜在的不安全行为需要严格评估。虽然先前的基准测试试图评估代理的安全性，但大多数都存在不足，例如依赖模拟环境、狭窄的任务领域或不真实的工具抽象。我们引入OpenAgentSafety，这是一个用于评估代理在八个关键风险类别中的行为的全面且模块化框架。与先前的工作不同，我们的框架评估与真实工具交互的代理，包括网络浏览器、代码执行环境、文件系统、bash shell和消息平台；并且支持超过350个多轮、多用户任务，涵盖良性和对抗性用户意图。OpenAgentSafety的设计具有可扩展性，允许研究人员以最小的努力添加工具、任务、网站和对抗策略。它结合了基于规则的分析和LLM作为评估者，以检测明显和微妙的不安全行为。对五种著名LLM在代理场景中的实证分析表明，在51.2%的安全漏洞任务中，Claude-Sonnet-3.7存在不安全行为，而o3-mini则高达72.7%，这凸显了关键的安全漏洞，以及在实际部署之前需要更强的保障措施。<br><b>English Abstract:</b> Recent advances in AI agents capable of solving complex, everyday tasks, from scheduling to customer service, have enabled deployment in real-world settings, but their possibilities for unsafe behavior demands rigorous evaluation. While prior benchmarks have attempted to assess agent safety, most fall short by relying on simulated environments, narrow task domains, or unrealistic tool abstractions. We introduce OpenAgentSafety, a comprehensive and modular framework for evaluating agent behavior across eight critical risk categories. Unlike prior work, our framework evaluates agents that interact with real tools, including web browsers, code execution environments, file systems, bash shells, and messaging platforms; and supports over 350 multi-turn, multi-user tasks spanning both benign and adversarial user intents. OpenAgentSafety is designed for extensibility, allowing researchers to add tools, tasks, websites, and adversarial strategies with minimal effort. It combines rule-based analysis with LLM-as-judge assessments to detect both overt and subtle unsafe behaviors. Empirical analysis of five prominent LLMs in agentic scenarios reveals unsafe behavior in 51.2% of safety-vulnerable tasks with Claude-Sonnet-3.7, to 72.7% with o3-mini, highlighting critical safety vulnerabilities and the need for stronger safeguards before real-world deployment.</span></td>
            </tr>
        </tbody>
    </table>
</body>
</html>