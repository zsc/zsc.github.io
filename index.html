<!DOCTYPE html>
<html>
<head>
    <title>Dr. Shuchang ZHOU</title>
    <meta http-equiv=Content-Type content="text/html; charset=utf-8">
<style>
    #MyBody {
        margin-left: 200px;
        margin-top: 40px
    }
    #PageBody {
        margin-left: 200px;
        margin-right: 200px;
        margin-bottom: 100px;
        height: 800px;
    }
    #Notice {
        margin-left: 200px;
        margin-right: 200px;
        height: 12px;
    }
</style>
</head>

<body style="background-color=#ffffff;">
        <table id="MyBody">
            <tbody>
               <tr>
                <td width="20"></td>
                <td width="260">
                    <img src="zsc.jpg" width="150" height="200" alt="Shuchang Zhou's Picture" />
                </td>
                <td width="40"></td>
                <td>
                    ZHOU Shuchang (周舒畅)
                    <br />Ph D,
                    <br />
                    <address><a href="mailto:shuchang [dot] zhou [at] gmail.com">shuchang [dot] zhou [at] gmail.com</a></address> 
                    <br />
	            I graduated from Tsinghua University in 2004 and obtained my PhD from the Chinese Academy of Sciences.
		    <br />
                </td>
                </tr>
            </tbody>
        </table>
        <div id="PageBody">
            <br />
            <hr />
            <h2> Talks </h2>
            <ul>
		<li> Self Driving by World Modelling (<a href="https://github.com/zsc/zsc.github.io/blob/master/Self%20Driving%20by%20World%20Modelling.pdf">PDF</a>), Oct. 2024 </li>
		<li> Code-centric Large Language Model (<a href="https://github.com/zsc/zsc.github.io/blob/master/code-centric-llm-v2.pdf">PDF</a>), Tsinghua University, May 2023 </li>
		<li> Hardware-software co-design for Computer Vision（<a href="https://github.com/zsc/zsc.github.io/blob/master/Hardware-software%20co-design%20for%20Computer%20Vision.pdf">PDF</a>, <a href="https://github.com/zsc/zsc.github.io/blob/master/V2%20Hardware-software%20codesign%20for%20Computer%20Vision.pdf">MNISC </a>），Tsinghua University, Apr. 2022 </li>
		<li> Coevolution of Neural Network and Computer Architecture (<a href="Coevolution of Neural Network and Computer Architecture(3).pdf"> PDF </a>), Aug. 2019</li>
		<li> Speculations about Computer Architecture in Next Three Years (<a href="Speculations+about+Computer+Architecture+in+Next+Three+Years.pdf"> PDF</a>), Jan. 20, 2018 </li>
		<li> Quantum Computing with Haskell and FPGA simulation (<a href="Quantum Computing with Haskell and FPGA simulation.pdf" target="_blank"> PDF </a>, <a href="https://github.com/zsc/qubit-fpga"> GitHub </a>), Jan. 18, 2018</li>
		<li> Smart Embedded Vision with Quantized Neural Network (<a href = "Smart Embedded Vision with Low Bitwidth Networks.pdf" > PDF </a>), Tsinghua University, Jul. 8 2017 </li>
                <li> Practical Methodology in Deep Learning (<a href="Practical Methodology in Deep Learning.pdf"  target="_blank"> PDF </a>), Peking University, Apr. 2017</li>
                <li> Neural Network Approximations (<a href="Neural Network Approximation.pdf"  target="_blank"> PDF </a>), Yao Class, Tsinghua University, Nov. 2016 </li>
		<li> Pointer Level Analysis (<a href="https://github.com/zsc/zsc.github.io/blob/fde85733a7bab395c4f932c0c4174f31eab7d1f3/Pointer%20level%20analysis.pdf"  target="_blank"> PDF </a>), 2009 </li>
            </ul>

            <hr />
            <h2> Selected Publications</h2>
            <ul>
		<li> (2025) Step-audio: Unified understanding and generation in intelligent speech interaction </li>
		<li> (ECCV'24) Chat-edit-3d: Interactive 3d scene editing via text prompts </li>
		<li> (ICCAD'23) Sole: Hardware-software co-design of softmax and layernorm for efficient transformer inference </li>
		<li> (AAAI'23) One is all: Bridging the gap between neural radiance fields architectures with progressive volume distillation </li>
		<li> (2023) Occdepth: A depth-aware method for 3d semantic scene completion </li>
	        <li> (ICCV'23) Occ$^2$Net: Robust Image Matching Based on 3D Occupancy Estimation for Occluded Regions </li>
		<li> (CVPR'23 highlight) Xiaotao Hu, et al. "A Dynamic Multi-Scale Voxel Flow Network for Video Prediction." (<a href="https://arxiv.org/abs/2303.09875"> arxiv</a>, <a href="https://github.com/megvii-research/CVPR2023-DMVFN"> github</a>) </li>		    
		<li> (CVPR'23 highlight) Shengchao Zhou, et al. "UniDistill: A Universal Cross-Modality Knowledge Distillation Framework for 3D Object Detection in Bird’s-Eye View." (<a href="https://arxiv.org/abs/2303.15083"> arxiv</a>, <a href="https://github.com/megvii-research/CVPR2023-UniDistill"> github</a>)  </li>
		<li> (CVPR'23) Yun-Hao Cao, et al. "Three Guidelines You Should Know for Universally Slimmable Self-Supervised Learning." (<a href="https://arxiv.org/abs/2303.06870"> arxiv</a>, <a href="https://github.com/megvii-research/US3L-CVPR2023"> github</a>) </li>
		<li> (AAAI oral) Shuangkang Fang, et al. "One is All: Bridging the Gap Between Neural Radiance Fields Architectures with Progressive Volume Distillation." (<a href="https://arxiv.org/abs/2211.15977"> arxiv</a>, <a href="https://github.com/megvii-research/AAAI2023-PVD"> github</a>) </li>
		<li> (ECCV oral) Yun-Hao Cao, Peiqin Sun, Yechang Huang, Jianxin Wu and Shuchang Zhou: Synergistic Self-supervised and Quantization Learning. (<a href="https://arxiv.org/abs/2207.05432"> arxiv</a>, <a href="https://github.com/megvii-research/SSQL-ECCV2022"> github</a>) </li>
		<li> (ECCV) Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, Shuchang Zhou: RIFE: Real-Time Intermediate Flow Estimation for Video Frame Interpolation. (<a href="https://arxiv.org/abs/2011.06294"> arxiv</a>, <a href="https://github.com/hzwer/arXiv2020-RIFE"> github</a>) </li>
		<li> (EACL) Yuekai Zhao, et al. Multi-split Reversible Transformers Can Enhance Neural Machine Translation </li>
		<li> (IJCAI) Yang Lin, et al. "Fq-vit: Fully quantized vision transformer without retraining". (<a href="https://arxiv.org/abs/2111.13824"> arxiv</a>, <a href="https://github.com/megvii-research/FQ-ViT"> github</a>) </li>   
		<li> (EMNLP) Yuekai Zhao, et al. Active learning approaches to enhancing neural machine translation. (<a href="https://aclanthology.org/2020.findings-emnlp.162.pdf"> pdf </a>) </li>   
		<li> (CVPR) Peibin Chen, et al. Data-efficient semi-supervised learning by reliable edge mining </li>
		<li> (ICCV) Zhewei Huang, Wen Heng, Shuchang Zhou: Learning to Paint with Model-based Deep Reinforcement Learning. (<a href="https://arxiv.org/abs/1903.04411"> arxiv</a>, <a href="https://github.com/megvii-research/ICCV2019-LearningToPaint"> github</a>) </li>		    
                <li> (CVPR) Xinyu Zhou, Cong Yao, He Wen, Yuzhi Wang, Shuchang Zhou, Weiran He, Jiajun Liang:
EAST: An Efficient and Accurate Scene Text Detector. (<a href="https://arxiv.org/abs/1704.03155"> arxiv</a>) </li>
                <li> (BMVC) Shuchang Zhou, Taihong Xiao, Yi Yang, Dieqiao Feng, Qinyao He, Weiran He: GeneGAN: Learning Object Transfiguration and Attribute Subspace from Unpaired Data. (<a href="https://arxiv.org/abs/1705.04932"> arxiv </a>, <a href="GeneGAN-BMVC2017.pdf"> slide </a>,<a href="https://github.com/megvii-research/GeneGAN"> GitHub </a>) </li>
                <li> Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, Yuheng Zou: DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients. (<a href="https://arxiv.org/abs/1606.06160"> arxiv </a>, <a href="https://github.com/ppwwyyxx/tensorpack/tree/master/examples/DoReFa-Net"> GitHub </a>) </li>
		<li> (ACL) Fangtao Li et al. "Deceptive answer prediction with user preference graph" </li>    
                <li> Shuchang Zhou: An Efficient Simulation Algorithm for Cache of Random Replacement Policy. <a href="https://hal.inria.fr/hal-01054982/document">PDF</a>, <a href="npc_sim_cache.pdf"> slide </a>, <a href="https://github.com/zsc/sim-cache">GitHub</a> </li>
		<li> Open64 on MIPS: porting and enhancing Open64 for Loongson II </li>    
                <li> <a href="https://scholar.google.com/citations?user=zYI0rysAAAAJ&hl=en"> More at Google Scholar (citation 7000+) </a> </li>
            </ul>

            <hr />

			<h2>Competitions</h2>
			<ul>
				<li> 1st place in the listening head generation track and 2nd place in the talking head generation track, ACM Multimedia ViCo 2022 Conversational Head Generation Challenge, <a href=https://arxiv.org/pdf/2206.12837.pdf> report </a> </li>
				<li> "Nuri" won the 1st place in <a href="https://www.ecole.ai/2021/ml4co-competition/#leaderboard"> NeurIPS'21 Machine Learning for Combinatorial Optimization </a> Dual Task track. <a href=https://github.com/megvii-research/NeurIPS2021-ML4CO-KIDA>Code</a> </li>				
				<li> "Megvii-hzwer" won the 2nd place in <a href="https://www.crowdai.org/challenges/nips-2017-learning-to-run/leaderboards"> NIPS'17 Learning to Run Challenge </a>, a competition on teaching a skeleton to run as fast as possible. We proposed the Actor-Critic Ensemble (ACE) method (<a href="https://arxiv.org/abs/1712.08987"> PDF </a>, <a href="https://github.com/hzwer/NIPS2017-LearningToRun"> Github</a>). </li>
				<li> "Megvii" won the 1st place in all tracks in<a href="http://nvlpubs.nist.gov/nistpubs/ir/2017/NIST.IR.8199.pdf"> NIST TRAIT '16 </a>, a competition on Text Recognition in the wild (OCR). </li>
			</ul>
			<hr />

            <h2>Research Areas</h2>
            <ul>
                <li>Machine Learning and Artificial Intelligence</li>
                <li>Computer Architecture</li>
                <li>Stochastic Optimization </li>
            </ul>

            <hr />
	    <h2>Academic Service</h2>
		<li> (in Chinese) Co-organize a joint course for three years on <a href="https://github.com/megvii-research/megvii-pku-dl-course">Deep Learning and Computer Vision</a> at Peking University.</li>
		<li> (in Chinese) Lectures at Tsinghua University on CV and Quantized Neural Network <a href=https://github.com/megvii-research/megvii-tsinghua-dl-course>link </a> </li>
		<li> Serving on <a href=https://jmlr.org/editorial-board-reviewers.html>JMLR editorial board</a>. </li>
            <hr />
            <!-- hhmts start -->Last update: Mar., 2025.
        </div>

</body>

</html>

