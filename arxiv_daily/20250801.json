{
  "papers": [
    {
      "id": "arXiv:2507.23704",
      "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction",
      "chinese_title": "用于高斯视频重建的增强速度场建模",
      "authors": "Zhenyang Li, Xiaoyang Bai, Tongchen Zhang, Pengfei Shen, Weiwei Xu, Yifan Peng",
      "abs_link": "https://www.google.com/url?q=https://arxiv.org/abs/2507.23704&sa=D&source=editors&ust=1754364653392474&usg=AOvVaw3ed0bxXot9FsPXhvMLV0OO",
      "pdf_link": "https://www.google.com/url?q=https://arxiv.org/pdf/2507.23704&sa=D&source=editors&ust=1754364653392560&usg=AOvVaw29yKVtBzx7f6wBO23EvM5R",
      "chinese_abstract": "摘要：高保真三维视频重建对于在虚拟和增强现实（VR/AR）中实现具有逼真运动的动态场景的实时渲染至关重要。3D高斯溅射的形变场范式因其深度形变网络强大的表示能力，在视频重建中已取得近乎照片般真实的效果。然而，在具有复杂运动和显著尺度变化的视频中，形变网络常对不规则的高斯轨迹过拟合，导致视觉质量不佳。此外，为静态场景重建设计的基于梯度的致密化策略不足以解决动态内容缺失的问题。鉴于这些挑战，我们提出了一种专为高斯视频重建量身定制的、由光流赋能的速度场建模方案，名为FlowGaussian-VR。它包含两个核心组件：一个支持基于光流优化的速度场渲染（VFR）管线，以及一个在动态区域调整高斯函数数量和大小的流辅助自适应致密化（FAD）策略。我们在包含挑战性运动场景的多个真实世界数据集上验证了我们模型在多视图动态重建和新视角合成方面的有效性，结果不仅显示了显著的视觉改进（PSNR增益超过2.5 dB）和动态纹理中更少的模糊伪影，还展示了正则化且可追踪的逐高斯轨迹。"
    },
    {
      "id": "arXiv:2507.23773",
      "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model",
      "chinese_title": "SimuRA：通过基于LLM世界模型的模拟推理架构实现通用目标导向智能体",
      "authors": "Mingkai Deng, Jinyu Hou, Yilin Shen, Hongxia Jin, Graham Neubig, Zhiting Hu, Eric Xing",
      "abs_link": "https://www.google.com/url?q=https://arxiv.org/abs/2507.23773&sa=D&source=editors&ust=1754364653378423&usg=AOvVaw1YmLfOCOsazitrGYk8YeCN",
      "pdf_link": "https://www.google.com/url?q=https://arxiv.org/pdf/2507.23773&sa=D&source=editors&ust=1754364653378644&usg=AOvVaw2LfzXFWfauxyqdv1tSW2dD",
      "chinese_abstract": "摘要：基于大型语言模型（LLMs）构建的AI智能体前景广阔，但当前实践侧重于“一个任务一个智能体”的方法，这不仅在可扩展性和通用性上有所欠缺，还受到自回归LLM的根本限制。另一方面，人类是通用智能体，通过心智模拟其行动和计划的结果进行推理。为了迈向更通用、更强大的AI智能体，我们引入了SimuRA，一个用于通用智能体推的目标导向架构。基于对任何环境中优化智能体的原则性表述，SimuRA通过引入一个用于模拟规划的世界模型，克服了自回归推理的局限性。该通用世界模型使用LLM实现，能够利用自然语言概念丰富的潜在空间，在广泛的环境中进行灵活规划。在困难的网页浏览任务上的实验表明，SimuRA将航班搜索的成功率从0%提高到32.2%。特别是，基于世界模型的规划比自回归规划表现出高达124%的持续优势，证明了世界模型模拟作为一种推理范式的优越性。我们对基于LLM训练单个通用智能体模型，使其能在所有环境中表现出超智能的可能性感到兴奋。作为开端，我们将基于SimuRA和预训练LLM构建的网页浏览智能体作为研究演示，供公众测试。"
    },
    {
      "id": "arXiv:2507.23698",
      "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents",
      "chinese_title": "可扩展多任务强化学习用于视觉-运动智能体的通用空间智能",
      "authors": "Shaofei Cai, Zhancun Mu, Haiwen Xia, Bowei Zhang, Anji Liu, Yitao Liang",
      "abs_link": "https://www.google.com/url?q=https://arxiv.org/abs/2507.23698&sa=D&source=editors&ust=1754364653393016&usg=AOvVaw3SNsiu9PAFKqAvYoODFQY9",
      "pdf_link": "https://www.google.com/url?q=https://arxiv.org/pdf/2507.23698&sa=D&source=editors&ust=1754364653393102&usg=AOvVaw1fLCjAFs_5t6yZR6jGKRmP",
      "chinese_abstract": "摘要：尽管强化学习（RL）在语言建模方面取得了显著成功，但其胜利尚未完全转化为视觉-运动智能体。RL模型的一个主要挑战是它们倾向于对特定任务或环境过拟合，从而阻碍了在不同设置下获得可泛化的行为。本文通过证明在《我的世界》中经过RL微调的视觉-运动智能体可以实现对未见世界的零样本泛化，为这一挑战提供了初步答案。具体来说，我们探索了RL在增强3D世界中可化空间推理和交互能力方面的潜力。为了解决多任务RL表示中的挑战，我们分析并建立了跨视角目标规范，作为视觉-运动策略的统一多任务目标空间。此外，为克服手动任务设计的重大瓶颈，我们提出了在高度可定制的《我的世界》环境中自动合成任务，以进行大规模多任务RL训练，并构建了一个高效的分布式RL框架来支持此过程。实验结果表明，RL将交互成功率显著提高了4倍，并使空间推理能够在包括现实世界在内的不同环境中实现零样本泛化。我们的发现强调了在3D模拟环境中进行RL训练的巨大潜力，特别是那些适合大规模任务生成环境，对于显著提升视觉-运动智能体的空间推理能力至关重要。"
    },
    {
      "id": "arXiv:2507.23540",
      "title": "A Unified Perception-Language-Action Framework for Adaptive Autonomous Driving",
      "chinese_title": "用于自适应自动驾驶的统一感知-语言-行框架",
      "authors": "Yi Zhang, Erik Leo Haß, Kuo-Yi Chao, Nenad Petrovic, Yinglei Song, Chengdong Wu, Alois Knoll",
      "abs_link": "https://www.google.com/url?q=https://arxiv.org/abs/2507.23540&sa=D&source=editors&ust=1754364653398549&usg=AOvVaw0b8OQ6bdCh4aU_K4m3n2ZR",
      "pdf_link": "https://www.google.com/url?q=https://arxiv.org/pdf/2507.23540&sa=D&source=editors&ust=1754364653398639&usg=AOvVaw1f_Ttj2krxAhnEciWIjdnn",
      "chinese_abstract": "摘要：自动驾驶系统在复杂、开放世界的环境中实现类人适应性、鲁棒性和可解释性方面面临重大挑战。这些挑战源于碎片化的架构、对新场景的泛化能力有限以及从感知中提取的语义不足。为了解决这些限制，我们提出了一个统一的感知-语言-行动（PLA）框架，该框架将多传感器融合（摄像头、激光雷达、雷达）与大型语言模型（LLM）增强的视觉-语言-行动（VLA）架构相结合，特别是一个由GPT-4.1驱动的推理核心。该架统一了低级感官处理与高级情境推理，将感知与基于自然语言的语义理解和决策制定紧密耦合，以实现情境感知、可解释且有安全边界的自动驾驶。在带有施工区的城市交叉口场景下的评估表明，该框架在轨迹跟踪、速度预测和自适应规划方面表现优越。结果凸显了语言增强认知框架在提升自动驾驶系统安全性、可解释性和可扩展性方面的潜力。"
    },
    {
      "id": "arXiv:2507.23318",
      "title": "FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning",
      "chinese_title": "FastDriveVLA：通过即插即用的基于重建的令牌剪枝实现高效的端到端驾驶",
      "authors": "Jiajun Cao, Qizhe Zhang, Peidong Jia, Xuhui Zhao, Bo Lan, Xiaoan Zhang, Xiaobao Wei, Sixiang Chen, Zhuo Li, Yang Wang, Liyun Li, Xianming Liu, Ming Lu, Shanghang Zhang",
      "abs_link": "https://www.google.com/url?q=https://arxiv.org/abs/2507.23318&sa=D&source=editors&ust=1754364653409167&usg=AOvVaw2GWPXQ3oL-3dg3H_1TuJX0",
      "pdf_link": "https://www.google.com/url?q=https://arxiv.org/pdf/2507.23318&sa=D&source=editors&ust=1754364653409256&usg=AOvVaw3qsSg3qSPUhp3FcrOUNRb-",
      "chinese_abstract": "摘要：视觉-语言-行动（VLA）模型在复杂场景理解和行动推理方面展现了巨大潜力，因此越来越多地被应用于端到端自动驾驶系统。然而，VLA模型的长视觉令牌（token）极大地增加了计算成本。当前视觉语言模型（VLM）中的视觉令牌剪枝方法要么依赖于视觉令牌相似性，要么依赖于视觉-文本注意力，但这两种方法在自动驾驶场景中均表现不佳。鉴于人类驾驶员在驾驶时会专注于相关的前景区域，我们认为保留包含这些前景信息的视觉令牌对于有效决策至关重要。受此启发，我们提出了FastDriveVLA，一个专为自动驾驶设计的、基于重建的新型视觉令牌剪枝框架。FastDriveVLA包含一个名ReconPruner的即插即用视觉令牌剪枝器，它通过MAE风格的像素重建来优先处理前景信息。我们设计了一种新颖的对抗性前景-背景重建策略来为VLA模型的视觉编码器训练ReconPruner。一旦训练完成，ReconPruner可以无缝应用于具有相同视觉编码器的不同VLA模型，无需重新训练。为了训练ReconPruner，我们还引入了一个名为nuScenes-FG的大规模数据集，包含24.1万个带有标注前景区域的图像-掩码对。我们的方法在nuScenes闭环规划基准测试中，在不同剪枝率下均取得了最先进的结果。"
    },
    {
      "id": "arXiv:2507.23067",
      "title": "FairReason: Balancing Reasoning and Social Bias in MLLMs",
      "chinese_title": "FairReason：在多模态大语言模型中平衡推理能力与社会偏见",
      "authors": "Zhenyu Pan, Yutong Zhang, Jianshu Zhang, Haoran Lu, Haozheng Luo, Yuwei Han, Philip S. Yu, Manling Li, Han Liu",
      "abs_link": "https://www.google.com/url?q=https://arxiv.org/abs/2507.23067&sa=D&source=editors&ust=1754364653388524&usg=AOvVaw1rHAuzF7_qfjMfivqc0q37",
      "pdf_link": "https://www.google.com/url?q=https://arxiv.org/pdf/2507.23067&sa=D&source=editors&ust=1754364653388614&usg=AOvVaw3GxdN8IY36R8-XyC-qUA8Q",
      "chinese_abstract": "摘要：多模态大型语言模型（MLLM）已在广泛的任务和模态中取得了最先进的成果。为了进一步提升其推理能力，近期的研究探索了先进的提示方案和后训练微调。尽管这些技术提高了逻辑准确性，但它们常常使模型的输出带有明显的社会偏见。因此，阐明推理能力的提升如何与偏见缓解相互作用——以及这两个目标是否内在权衡——仍然是一个开放且紧迫的研究问题。我们的研究首先在相同条件下对三种偏见缓解策略——监督式微调（SFT）、知识蒸馏（KD）和基于规则的强化学习（RL）——进行基准测试，确立它们各自的基线优势和劣势。在此基上，我们通过改变每种范式中去偏见和推理中心样本的比例，来描绘推理与偏见的权衡曲线。我们的扫描实验揭示了一个一致的“甜蜜点”：采用强化学习训练的约1:4混合比例，能将刻板印象分数降低10%，同时保留模型88%的原始推理准确率，为在MLLM中平衡公平性与能力提供了具体的指导。"
    },
    {
      "id": "arXiv:2507.23058",
      "title": "Reference-Guided Diffusion Inpainting For Multimodal Counterfactual Generation",
      "chinese_title": "用于多模态反事实生成的参考引导扩散修复",
      "authors": "Alexandru Buburuzan",
      "abs_link": "https://www.google.com/url?q=https://arxiv.org/abs/2507.23058&sa=D&source=editors&ust=1754364653420954&usg=AOvVaw2pj_nXYtrORJmf4sUg4CuK",
      "pdf_link": "https://www.google.com/url?q=https://arxiv.org/pdf/2507.23058&sa=D&source=editors&ust=1754364653421038&usg=AOvVaw26teD40Dw8rH8FwYiEXYDk",
      "chinese_abstract": "摘要：安全关键型应用，如自动驾驶和医学图像分析，需要大量的多模态数据进行严格测试。由于收集真实世界数据的成本和复杂性，合成数据方法日益受到重视，但它们需要高度的真实性和可控性才能发挥作用。本文介绍了两种用于自动驾驶和医学图像分析中合成数据生成的新方法，分别为MObI和AnydoorMed。MObI是首个多模态物体修复（Multimodal Object Inpainting）框架，利用扩散模型在多种感知模态（同时在摄像头和激光雷达上展示）上生成真实且可控的物体修复。给定单个参考RGB图像，MObI能够在指定的三维位置，通过边界框引导，无缝地将物体插入到现有的多模态场景中，同时保持语义一致性和多模态连贯性。与仅依赖编辑掩码的传统修复方法不同，该方法使用三维边界框条件来确保准确的空间定位和逼真的缩放。AnydoorMed将此范式扩展到医学成像领域，专注于乳腺X光扫描的参考引导修复它利用基于扩散的模型对异常进行修复，并能出色地保留细节，维持参考异常的结构完整性，同时在语义上与周围组织融合。总之，这些方法表明，用于自然图像中参考引导修复的基础模型可以轻松适应不同的感知模态，为下一代能够构建高度真实、可控和多模态反事实场景的系统铺平了道路。"
    },
    {
      "id": "arXiv:2507.23010",
      "title": "Investigating the Invertibility of Multimodal Latent Spaces: Limitations of Optimization-Based Methods",
      "chinese_title": "探究多模态潜在空间的可逆性：基于优化方法的局限性",
      "authors": "Siwoo Park",
      "abs_link": "https://www.google.com/url?q=https://arxiv.org/abs/2507.23010&sa=D&source=editors&ust=1754364653422926&usg=AOvVaw0DuhuxeUjuK206TpUNFpxN",
      "pdf_link": "https://www.google.com/url?q=https://arxiv.org/pdf/2507.23010&sa=D&source=editors&ust=1754364653423014&usg=AOvVaw0NrEr1LaJbgUXTdfccScJF",
      "chinese_abstract": "摘要：本文研究了特定任务人工智能（AI）模型中多模态潜在空间的反向能力和更广泛的效用。虽然这些模型在其设计的正向任务（如文本到图像生成、音频到文本转录）中表现出色，但它们在反向映射方面的潜力在很大程度上仍未被探索。我们提出了一个基于优化的框架，从期望的输出中推断输入特征，并将其双向应用于文本-图像（BLIP, Flux.1-dev）和文本-音频（Whisper-Large-V3, Chatterbox-TTS）模态。我们的核心假设是，虽然优化可以引导模型进行反向任务，但它们的多模态潜在空间不会持续支持具有语义意义和感知连贯性的反向映射。实验结果一致验证了这一假设。我们证明，尽管优化可以迫使模型产生在文本上与目标一致的输出（例如，文本到图像模型生成的图像能被图像字幕模型正确描述，或者ASR模型能准确转录优化后的音频），但这些反向生成的感知质是混乱且不连贯的。此外，当试图从生成模型中推断原始语义输入时，重建的潜在空间嵌入经常缺乏语义可解释性，与无意义的词汇标记对齐。这些发现突显了一个关键的局限性：主要为特定正向任务优化的多模态潜在空间，本身不具备支持稳健且可解释的反向映射所需的结构。我们的工作强调了进一步研究开发真正具有语义丰富性和可逆性的多模态潜在空间的必要性。"
    },
    {
      "id": "arXiv:2507.22920",
      "title": "Discrete Tokenization for Multimodal LLMs: A Comprehensive Survey",
      "chinese_title": "面向多模态大语言模型的离散化分词：一项综合综述",
      "authors": "Jindong Li, Yali Fu, Jiahong Liu, Linxiao Cao, Wei Ji, Menglin Yang, Irwin King, Ming-Hsuan Yang",
      "abs_link": "https://www.google.com/url?q=https://arxiv.org/abs/2507.22920&sa=D&source=editors&ust=1754364653433104&usg=AOvVaw27Y_FiUlxG9JPPUezZmii5",
      "pdf_link": "https://www.google.com/url?q=https://arxiv.org/pdf/2507.22920&sa=D&source=editors&ust=1754364653433196&usg=AOvVaw3z7m2VmRjzA5fqpA6fodLd",
      "chinese_abstract": "摘要：大型语言模型（LLM）的快速发展加剧了对有效机制的需求，以将连续的多模态数据转换为适合基于语言处理的离散表示。以向量量化（VQ）为核心方法的离散化分词（tokenization），既提供了计算效率，又与LLM架构兼容。尽管其重要性日益增加，但目前缺乏一篇全面综述来系统地审视基于LLM系统中的VQ技术。本工作填补了这一空白，首次提出了专为LLM设计的离散化分词方法的结构化分类法和分析。我们对跨越经典和现代范式的8种代表性VQ变体进行了分类，并分析了它们的算法原理、训练动态以及与LLM流水线的集成挑战。除了算法层面的研究，我们还从经典非LLM应用、基于LLM的单模态系统和基于LLM的多模态系统三个方面讨论了现有研究，强调了量化策略如何影响对齐、推理和生成性能。此外，我们识别了包括码本崩溃、梯度估计不稳定和特定模态编码约束在内的关键挑战。最后，我们讨论了新兴的研究方向，如动态和任务自适应量化、统一的分词框架以及受生物启发的码本学习。本综述弥合了传统向量量化与现代LLM应用之间的差距，为开发高效且可泛化的多模态系统提供了基础性参考。持续更新的版本可在此 https URL 查看。"
    },
    {
      "id": "arXiv:2507.22940",
      "title": "Trustworthy Reasoning: Evaluating and Enhancing Factual Accuracy in LLM Intermediate Thought Processes",
      "chinese_title": "可信推理：评估和增强LLM中间思维过程中的事实准确性",
      "authors": "Rui Jiao, Yue Zhang, Jinku Li",
      "abs_link": "https://www.google.com/url?q=https://arxiv.org/abs/2507.22940&sa=D&source=editors&ust=1754364653425936&usg=AOvVaw0AMKcPjrS1o3FiXLAV2NVN",
      "pdf_link": "https://www.google.com/url?q=https://arxiv.org/pdf/2507.22940&sa=D&source=editors&ust=1754364653426021&usg=AOvVaw2gW1d6Tfb59s1LLvNq-pw8",
      "chinese_abstract": "摘要：我们提出了RELIANCE（为增强置信度而进行的逻辑完整性与准确性推理评估）框架，这是一个旨在解决大型语言模型（LLM）中一个关键漏洞的新型框架：即尽管最终答案正确，但在中间推理步骤中普遍存在事实不准确性。这种现象在医疗保健、法律分析和科学研究等高风险领域构成了重大风险，因为错误但自信呈现的推理可能误导用户做出危险的决策。我们的框架集成了三个核心组件：（1）一个专门的事实核查分类器，该分类器在反事实增强数据上进行训练，以检测推理链中微妙的事实不一致性；（2）一种组相对策略优化（GRPO）强化学习方法，通过多维奖励平衡事实性、连贯性和结构正确性；（3）一个机理可解释性模块，用于研究事实性改进在推理程中如何体现在模型激活中。对十个最先进模型的广泛评估揭示了令人担忧的模式：即使是像Claude-3.7和GPT-o1这样的领先模型，其推理事实准确率也分别只有81.93%和82.57%。RELIANCE显著增强了事实的鲁棒性（提升高达49.90%），同时在包括Math-500、AIME-2024和GPQA在内的挑战性基准上保持或提高了性能。此外，我们的激活层面分析为事实性增强如何重塑模型架构内的推理轨迹提供了可行的见解，为未来通过激活引导优化明确针对事实鲁棒性的训练方法奠定了基础。"
    },
    {
      "id": "arXiv:2507.23726",
      "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving",
      "chinese_title": "Seed-Prover：用于自动定理证明的深度与广度推理",
      "authors": "Luoxin Chen, Jinming Gu, Liankai Huang, Wenhao Huang, Zhicheng Jiang, Allan Jie, Xiaoran Jin, Xing Jin, Chenggang Li, Kaijing Ma, Cheng Ren, Jiawei Shen, Wenlei Shi, Tong Sun, He Sun, Jiahui Wang, Siran Wang, Zhihong Wang, Chenrui Wei, Shufa Wei, Yonghui Wu, Yuchen Wu, Yihang Xia, Huajian Xin, Fan Yang, Huaiyuan Ying, Hongyi Yuan, Zheng Yuan, Tianyang Zhan, Chi Zhang, Yue Zhang, Ge Zhang, Tianyun Zhao, Jianqiu Zhao, Yichi Zhou, Thomas Hanwen Zhu",
      "abs_link": "https://www.google.com/url?q=https://arxiv.org/abs/2507.23726&sa=D&source=editors&ust=1754364653379553&usg=AOvVaw2ck6NGKsE5q77C2Dt-WhcE",
      "pdf_link": "https://www.google.com/url?q=https://arxiv.org/pdf/2507.23726&sa=D&source=editors&ust=1754364653379652&usg=AOvVaw2UyC2fUaZVmc7yvw1oirz7",
      "chinese_abstract": "摘要：大型语言模型（LLM）通过利用强化学习和长思维链，已展现出强大的数学推理能力，但由于在仅使用自然语言时缺乏明确的监督信号，它们在定理证明方面仍然面临困难。像Lean这样的专用领域特定语言通过对证明进行形式化验证提供了明确的监督，从而能够通过强化学习进行有效训练。在这项工作中，我们提出了Seed-Prover，一个引理式全证明推理模型。Seed-Prover可以根据Lean的反馈、已证明的引理和自我总结来迭代地优化其证明。为了解决IMO级别的竞赛问题，我们设计了三种测试时推理策略，以实现深度和广度推理。Seed-Prover证明了78.1%的形式化IMO历史问题，在MiniF2F上达到饱和，并在PutnamBench上取得了超过50%的成绩，大幅超越了先前的最先进水平。为了解决Lean中缺乏几何支持的问题，我们引入了一个几何推理引擎Seed-Geometry，其性能优于之前的形式化几何引擎。我们使用这两个系统参加了IMO 2025，并完全证明了6个问题中的5个。这项工作代表了自动数学推理领域的重大进展，展示了形式化验证与长思维链推理相结合的有效性。"
    },
    {
      "id": "arXiv:2507.23261",
      "title": "DynaSwarm: Dynamically Graph Structure Selection for LLM-based Multi-agent System",
      "chinese_title": "DynaSwarm：于基于LLM的多智能体系统的动态图结构选择",
      "authors": "Hui Yi Leong, Yuqing Wu",
      "abs_link": "https://www.google.com/url?q=https://arxiv.org/abs/2507.23261&sa=D&source=editors&ust=1754364653411748&usg=AOvVaw0WcD-Z3Y-hKjLkum0AmajR",
      "pdf_link": "https://www.google.com/url?q=https://arxiv.org/pdf/2507.23261&sa=D&source=editors&ust=1754364653411839&usg=AOvVaw114B4tEcIndZetFCOQNnV4",
      "chinese_abstract": "摘要：当前的多智能体系统（MAS）框架通常依赖于手动设计和静态的协作图结构，这限制了其适应性和性能。为了解决这些限制，我们提出了DynaSwarm，一个通过两项关键创新来增强基于LLM的MAS的动态框架：（1）一个行动者-评论家强化学习（A2C）机制，用于优化图结构，比先前的RL方法具有更好的稳定性；（2）一个动态图选择器，通过参数高效的LLM微调，为每个输入样本自适应地选择最优的图结构。DynaSwarm消除了对僵化、一刀切的图构的需求，而是利用特定于样本的特性来动态地通过专门的智能体网络路由查询。（c）我们建议微调演示检索器，以充分利用上下文学习（ICL）的力量。在问答、数学推理和编码任务上的广泛实验表明，DynaSwarm在多个LLM骨干上始终优于最先进的单智能体和MAS基线。我们的发现凸显了在LLM MAS设计中，样本感知的结构灵活性的重要性。"
    },
    {
      "id": "arXiv:2507.23682",
      "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models",
      "chinese_title": "villa-X：增强视觉-语言-行动模型中的潜在行动建模",
      "authors": "Xiaoyu Chen, Hangxing Wei, Pushi Zhang, Chuheng Zhang, Kaixin Wang, Yanjiang Guo, Rushuai Yang, Yucen Wang, Xinquan Xiao, Li Zhao, Jianyu Chen, Jiang Bian",
      "abs_link": "https://www.google.com/url?q=https://arxiv.org/abs/2507.23682&sa=D&source=editors&ust=1754364653393932&usg=AOvVaw1-FHPfA37cN_NZ336v7FA0",
      "pdf_link": "https://www.google.com/url?q=https://arxiv.org/pdf/2507.23682&sa=D&source=editors&ust=1754364653394028&usg=AOvVaw25qrImpVnvW6WUIOtnkSGD",
      "chinese_abstract": "摘要：视觉-语言-行动（VLA）模型已成为学习机器人操控策略的一种流行范式，这些策略能够遵循语言指令并泛化到新颖的场景。最近的工作已开始探索将潜在行动（两帧之间视觉变化的抽象表示）融入VLA预训练中。在本文中，我们介绍了villa-X，一个新颖的视觉-语言-潜在行动（ViLLA）框架，该框架推进了潜在行动建模，以学习可泛化的机器人操控策略。我们的方法改进了潜在行动的学习方式以及它们如何被整合到VLA预训练中。这些贡献共同使villa-X在包括SIMPLER和LIBERO在内的模拟环境中，以及在包括夹爪和灵巧手操控的两个真实世界机器人设置中取得了优越的性能。我们相信ViLLA范式具有巨大的潜力，而我们的villa-X为未来的研究提供了坚实基础。"
    },
    {
      "id": "arXiv:2507.23536",
      "title": "From LLMs to Edge: Parameter-Efficient Fine-Tuning on Edge Devices",
      "chinese_title": "从LLM到边缘：边缘设备上的参数高效微调",
      "authors": "Georg Slamanig, Francesco Corti, Olga Saukh",
      "abs_link": "https://www.google.com/url?q=https://arxiv.org/abs/2507.23536&sa=D&source=editors&ust=1754364653399044&usg=AOvVaw1nKwKyBMB_exbsFYzYZJQp",
      "pdf_link": "https://www.google.com/url?q=https://arxiv.org/pdf/2507.23536&sa=D&source=editors&ust=1754364653399135&usg=AOvVaw3Glq3kLLYXT1_MmGt6tu6h",
      "chinese_abstract": "摘要：参数高效微调（PEFT）方法通过最小化用于使模型适应下游任务的额外参数数量，来降低更新深度学习模型的计算成本。虽然在大语言模型（LLM）中得到了广泛研究，但其在边缘设备上使用的小型模型（如卷积神经网络）中的应用仍未得到充分探索。本文对通常部署在资源受限的边缘环境中卷积架构上的流行PEFT方法进行了基准测试和分析。我们评估了LoRA、DoRA和GaLore在更新标准和深度可分离卷积架构以处理分布偏移和适应未见类别方面的表现。我们利用最近提出的PyTorch分析器，比较了这些PEFT方法与传统微调方法在更新模型性能和计算成本上的差异。考虑到资源效率，我们研究了它们在不同秩维度下的更新行为。我们发现，所评估的PEFT方法在应用于深度可分离卷积架构时，其内存效率仅为应用于LLM时的一半。相反，当针对为边缘部署优化的卷积架构时，基于适配器的PEFT方法可以在模型更新期间将浮点运算（FLOPs）减少高达95%。这些见解为根据硬件限制、性能要求和应用需求选择PEFT方法提供了宝贵的指导。我们的代码已在线发布。"
    },
    {
      "id": "arXiv:2507.23091",
      "title": "Moravec's Paradox: Towards an Auditory Turing Test",
      "chinese_title": "莫拉维克悖论迈向听觉图灵测试",
      "authors": "David Noever, Forrest McKee",
      "abs_link": "https://www.google.com/url?q=https://arxiv.org/abs/2507.23091&sa=D&source=editors&ust=1754364653388041&usg=AOvVaw2a7uNajUt5bQTsaDmzx9si",
      "pdf_link": "https://www.google.com/url?q=https://arxiv.org/pdf/2507.23091&sa=D&source=editors&ust=1754364653388123&usg=AOvVaw3Mk6HlqbTqCv0_bUi6ksFV",
      "chinese_abstract": "摘要：本研究工作表明，当前的人工智能系统在人类能毫不费力完成的听觉任务上会遭遇灾难性的失败。受莫拉维克悖论（即对人类简单的任务对机器往往困难，反之亦然）的启发，我们引入了一个听觉图灵测试，包含七个类别的917个挑战：重叠语音、噪声中的语音、时间失真、空间音频、咖啡店噪声、电话失真和感知错觉。我们对包括GPT-4的音频能力和OpenAI的Whisper在内的最先进音频模型进行了评估，结果显示其失败率惊人地超过93%，即使是表现最好的型，在人类以7.5倍成功率（52%）解决的任务上，也仅实现了6.9%的准确率。这些结果暴露了AI系统在处理复杂听觉场景时的聚焦失败，特别是在选择性注意、噪声鲁棒性和情境适应方面。我们的基准测试不仅量化了人机听觉差距，还为这些失败发生的原因提供了见解，表明当前架构缺乏类人听觉场景分析的基本机制。传统音频验证码的设计突显了人类进化出而机器在多模态语言模型中未能选择的常见过滤器。这项工作建立了一个诊断框架，用于衡量迈向人类水平机器听觉的进展，并强调需要将选择性注意、基于物理的音频理解和情境感知集成到多模态AI系统中的新方法。"
    },
    {
      "id": "arXiv:2507.23511",
      "title": "MECAT: A Multi-Experts Constructed Benchmark for Fine-Grained Audio Understanding Tasks",
      "chinese_title": "MECAT：一个用于细粒度音频理解任务的多专家构建基准",
      "authors": "Yadong Niu, Tianzi Wang, Heinrich Dinkel, Xingwei Sun, Jiahao Zhou, Gang Li, Jizhong Liu, Xunying Liu, Junbo Zhang, Jian Luan",
      "abs_link": "https://www.google.com/url?q=https://arxiv.org/abs/2507.23511&sa=D&source=editors&ust=1754364653399952&usg=AOvVaw3DYgupxrLC9DnelLxXNdc5",
      "pdf_link": "https://www.google.com/url?q=https://arxiv.org/pdf/2507.23511&sa=D&source=editors&ust=1754364653400038&usg=AOvVaw3lh_GQvt0WLe412CxDepCC",
      "chinese_abstract": "摘要：尽管大型音频-语言模型在开放式音频理解方面取得了进展，但它们仍然未能达到人类细致入微的理解水平。这一差距之所以持续存在，很大程度上是因为当前的基准测试受限于数据标注和评估指标，无法可靠地区分通用输出和高度详细的输出。为此，本工作引入了MECAT，一个用于细粒度音频理解任务的多专家构建基准。MECAT通过一个集成了专业专家模型分析与大型语言模型思维链推理的流程生成，提供了视角、细粒度的音频描述和开放集问答对。该基准辅以一种新颖的评估指标：DATE（Discriminative-Enhanced Audio Text Evaluation）。该指标通过结合单样本语义相似度和跨样本可辨别性，惩罚通用术语并奖励详细描述。我们还对最先进的音频模型进行了全面评估，为它们当前的能力和局限性提供了新的见解。数据和代码可在此 https URL 获取。"
    },
    {
      "id": "arXiv:2507.23751",
      "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks",
      "chinese_title": "CoT-Self-Instruct：为推理和非推理任务构建高质量的合成提示",
      "authors": "Ping Yu, Jack Lanchantin, Tianlu Wang, Weizhe Yuan, Olga Golovneva, Ilia Kulikov, Sainbayar Sukhbaatar, Jason Weston, Jing Xu",
      "abs_link": "https://www.google.com/url?q=https://arxiv.org/abs/2507.23751&sa=D&source=editors&ust=1754364653379006&usg=AOvVaw01xBS5Hf4fFlRrlXnOCQZB",
      "pdf_link": "https://www.google.com/url?q=https://arxiv.org/pdf/2507.23751&sa=D&source=editors&ust=1754364653379100&usg=AOvVaw1FhS2e9QKPBnzw-iW0zbxz",
      "chinese_abstract": "摘要：我们提出了CoT-Self-Instruct，一种合成数据生成方法，该方法指导LLM首先基于给定的种子任务通过思维链（CoT）进行推理和规划，然后生成一个质量和复杂性相似的新合成提示用于LLM训练，最后通过自动度量筛选出高质量数据。在可验证的推理任务中，我们的合成数据在MATH500、AMC23、AIME24和GPQA-Diamond等数据集上的表现显著优于现有的训练数据集，如s1k和OpenMathReasoning。对于不可验证的指令遵循任务，我们的方法在AlpacaEval 2.0和Arena-Hard上的性能也超过了人类或标准自指令提示。"
    },
    {
      "id": "arXiv:2507.22928",
      "title": "How does Chain of Thought Think? Mechanistic Interpretability of Chain-of-Thought Reasoning with Sparse Autoencoding",
      "chinese_title": "思维是如何思考的？使用稀疏自编码对思维链推理进行机理可解释性分析",
      "authors": "Xi Chen, Aske Plaat, Niki van Stein",
      "abs_link": "https://www.google.com/url?q=https://arxiv.org/abs/2507.22928&sa=D&source=editors&ust=1754364653430457&usg=AOvVaw3oxThJjTVLug8KEYzrfb7y",
      "pdf_link": "https://www.google.com/url?q=https://arxiv.org/pdf/2507.22928&sa=D&source=editors&ust=1754364653430545&usg=AOvVaw0vtcPjCfToS9unL9hwLJzD",
      "chinese_abstract": "摘要：思维链（CoT）提示提升了大型语言模型在多步任务上的准确性，但生成的“思考”是否反映了真实的内部推理过程仍未解决。我们首次对CoT的忠实性进行了特征级别的因果研究。通过将稀疏自编码器与激活补丁（activation patching）相结合，我们从Pythia-70M和Pythia-2.8B模型中提取了单语义特征，而这些模型在CoT和普通（无CoT）提示下解决GSM8K数学问题。将一小组CoT推理特征交换到无CoT的运行中，显著提高了2.8B模型中的答案对数概率，但在70M模型中没有可靠效果，揭示了一个明显的规模阈值。在更大的模型中，CoT还导致了显著更高的激活稀疏性和特征可解释性分数，表明其内部计算更加模块化。例如，模型生成正确答案的置信度从1.2提高到4.3。我们引入了补丁曲线和随机特征补丁基线，表明有用的CoT信息不仅存在于前K个补丁中，而是广泛分布的。总的来说，我们的结果表明，CoT可以在高容量的LLM中诱导出更具可解释性的内部结构，验证了其作为一种结构化提示方法的作用。"
    },
    {
      "id": "arXiv:2507.23042",
      "title": "Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language Driving",
      "chinese_title": "用于实时视觉-语言驾驶的早期目标引导多尺度融合",
      "authors": "Santosh Patapati, Trisanth Srinivasan",
      "abs_link": "https://www.google.com/url?q=https://arxiv.org/abs/2507.23042&sa=D&source=editors&ust=1754364653421442&usg=AOvVaw3PJurYz0-bx61B7IrJdnue",
      "pdf_link": "https://www.google.com/url?q=https://arxiv.org/pdf/2507.23042&sa=D&source=editors&ust=1754364653421523&usg=AOvVaw3zFmNDm1NOTaSMlXDBeoZT",
      "chinese_abstract": "摘要：自动驾驶汽车必须在毫秒内做出反应，同时对道路几何结构和交通意图进行推理，以应对复杂情况。我们引入了NovaDrive，一个单分支的视觉-语言架构，它在单一分支中处理前置摄像头图像、高清地图瓦片、激光雷达深度和文本路径点。一个轻量级的、两阶段的交叉注意力模块首先将路径点令牌与高清地图对齐，然后在细粒度的图像和深度补丁上精炼注意力。结合一种新颖的平滑度损失函数，该函数不鼓励突然的转向和速度变化，这种设计消除了对循环记忆的需求。我们微调了一个11B参数的LLaMA-3.2视觉-语言骨干模型的顶层15层，以实现实时推理。在MD-NEX Outdoor基准测试的nuScenes/Waymo子集上，NovaDrive将成功率提高到84%（+4%），路径效率（SPL）提升至0.66（+0.11），并将碰撞频率从2.6%降低到1.2%（-1.4%），均优于先前的最先进技术。我们的消融实验证实，路径点令牌、部分VLM微调和交叉注意力融合各自对这些增益贡献最大。除了安全性，NovaDrive更短的路线（得益于新颖的平滑度损失）转化为更低的燃料或电池消耗，指向了更精简、更易于更新的驾驶技术栈。NovaDrive也可以扩展到其他具身AI领域。"
    },
    {
      "id": "arXiv:2507.23735",
      "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy",
      "chinese_title": "用于认知水下机器人自主性的分布式AI智能体",
      "authors": "Markus Buchholz, Ignacio Carlucho, Michele Grimaldi, Yvan R. Petillot",
      "abs_link": "https://www.google.com/url?q=https://arxiv.org/abs/2507.23735&sa=D&source=editors&ust=1754364653391945&usg=AOvVaw1fbWwar8Om4BxUZN-cKb3V",
      "pdf_link": "https://www.google.com/url?q=https://arxiv.org/pdf/2507.23735&sa=D&source=editors&ust=1754364653392037&usg=AOvVaw0KqD9TkKBeMCUD198eJEws",
      "chinese_abstract": "摘要：在复杂、不可预测的环境中导航的机器人实现鲁棒的认知自主性，仍然是机器人学的一个根本挑战。本文介绍了水下机器人自组织自主性（UROSA），这是一个开创性的架构，它利用集成在机器人操作系统2（ROS 2）框架内的分布式大型语言模型AI智能体，为自主水下航行器（AUV）提供先进的认知能力。UROSA将认知分散到专门的AI智能体中，这些智能体负责多模态感知、自适应推理、动态任务规划和实时决策。核心创新包括：灵活的智能体动态调整其角色、利用向量数据库进行高效知识管理的检索增强生成、强化学习驱动的行为优化，以及用于运行时功能可扩展性的自主即时ROS 2节点生成。广泛的实证验证通过在模拟和真实世界部署的现实水下任务，证明了UROSA在处理意外场景、环境不确定性和新任务目标方面，相比传统基于规则的架构具有显著优势，显示出其良好的适应性和可靠性。这项工作不仅推动了水下自主技术的发展，还建立了一个可扩展、安全且多功能的认知机器人框架，能够推广到各种现实世界的应用中。"
    }
  ],
  "clusters": {
    "LLM智能体与强化学习": [
      "arXiv:2507.23773",
      "arXiv:2507.23698",
      "arXiv:2507.23261",
      "arXiv:2507.23735",
      "arXiv:2507.23726"
    ],
    "多模态模型与自主驾驶": [
      "arXiv:2507.23540",
      "arXiv:2507.23318",
      "arXiv:2507.23042",
      "arXiv:2507.23682",
      "arXiv:2507.22920",
      "arXiv:2507.23010"
    ],
    "生成式AI、三维视觉与音频": [
      "arXiv:2507.23704",
      "arXiv:2507.23058",
      "arXiv:2507.23091",
      "arXiv:2507.23511"
    ],
    "LLM对齐、高效训练与可解释性": [
      "arXiv:2507.22940",
      "arXiv:2507.23067",
      "arXiv:2507.23751",
      "arXiv:2507.23536",
      "arXiv:2507.22928"
    ]
  }
}
