
                <!DOCTYPE html>
                <html lang="zh-CN">
                <head>
                    <meta charset="UTF-8">
                    <title>学术速递 - 2025-07-15</title>
                    <style>
        /* --- General & Layout --- */
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Helvetica Neue", "Arial", sans-serif; line-height: 1.6; color: #333; background-color: #f0f2f5; margin: 0; padding: 0; }
        .container { max-width: 1000px; margin: 20px auto; background-color: #fff; padding: 30px; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.08); }

        /* --- Config Panel --- */
        .config-panel { background-color: #f8f9fa; padding: 25px; border-radius: 8px; margin-bottom: 30px; border: 1px solid #e9ecef; }
        .config-panel h2 { margin-top: 0; color: #2c3e50; border-bottom: 2px solid #e0e0e0; padding-bottom: 10px; }
        .toggle-header { cursor: pointer; user-select: none; }
        .toggle-header::after { content: ' (点击展开/折叠)'; font-size: 0.75em; color: #6c757d; font-weight: normal; }
        .form-group { margin-bottom: 15px; }
        .form-group label { display: block; font-weight: bold; margin-bottom: 5px; color: #495057; }
        .form-group input[type="text"], .form-group input[type="password"], .form-group input[type="date"], .form-group textarea {
            width: 100%; padding: 10px; border: 1px solid #ced4da; border-radius: 4px; box-sizing: border-box; font-size: 1rem;
        }
        .form-group textarea { resize: vertical; min-height: 80px; }
        .btn {
            display: inline-block; padding: 12px 20px; font-size: 1rem; font-weight: bold; text-align: center; color: #fff;
            background-color: #007bff; border: none; border-radius: 4px; cursor: pointer; transition: background-color 0.2s;
        }
        .btn:hover { background-color: #0056b3; }
        .btn-download { background-color: #28a745; margin-left: 10px; display: none; }
        .btn-download:hover { background-color: #218838; }

        /* --- Report Display --- */
        #report-container h1, #report-container h2 { color: #2c3e50; border-bottom: 2px solid #e9ecef; padding-bottom: 10px; }
        #report-container h1 { text-align: center; }
        table { width: 100%; border-collapse: collapse; margin-top: 20px; }
        th, td { padding: 12px 15px; text-align: left; border-bottom: 1px solid #dee2e6; }
        th { background-color: #f2f2f2; }
        a { color: #007bff; text-decoration: none; }
        a:hover { text-decoration: underline; }
        .paper-title-row { cursor: pointer; transition: background-color 0.2s; }
        .paper-title-row:hover { background-color: #e9ecef; }
        .paper-title-row.expanded { background-color: #e2e6ea; }
        .paper-title { font-weight: bold; }
        .details-row { display: none; }
        .details-row.show { display: table-row; }
        .details-cell { padding: 20px 25px !important; background-color: #f8f9fa; border-left: 3px solid #007bff; }
        .authors { font-style: italic; color: #555; margin-top: 8px; display: block; }
        .abstract { margin-top: 10px; }
        #status { text-align: center; font-size: 1.1em; padding: 20px; background-color: #e9ecef; border-radius: 5px; margin-top: 20px; display: none; }
    </style>
                </head>
                <body>
                    
                <div class="container" id="report-container">
                    <h1>今日学术速递 (2025-07-15)</h1>
                    <p id="intro">为您找到日期 2025-07-15 的数据。论文已为您整理成以下 4 个主题。点击论文标题可展开查看摘要。</p>
                    <div id="clusters-content">
                    <section>
                        <h2>强化学习与模型对齐</h2>
                        <table>
                            <thead><tr><th>论文 (Paper)</th></tr></thead>
                            <tbody>
                        <tr class="paper-title-row">
                            <td>
                                <div class="paper-title">基于生成式AI的多智能体强化学习以实现分布式智能体智能：从生成式强化学习智能体的视角</div>
                                <div><a href="https://arxiv.org/pdf/2507.09495" target="_blank" onclick="event.stopPropagation();" title="Download PDF">arXiv:2507.09495 - GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective</a></div>
                            </td>
                        </tr>
                        <tr class="details-row">
                            <td class="details-cell">
                                <strong>作者 (Authors):</strong>
                                <div class="authors">Hang Wang, Junshan Zhang</div>
                                <strong style="margin-top:10px; display:block;">摘要 (Abstract):</strong>
                                <div class="abstract">多智能体强化学习面临着传统方法未能克服的根本性挑战：指数级增长的联合动作空间、同时学习导致目标移动的非平稳环境，以及限制协调的部分可观测性。现有方法仍然是被动式的，采用刺激-响应机制，在面对新颖场景时会失败。我们主张通过基于生成式AI的强化学习，实现从被动式到主动式多智能体智能的范式转变。本文主张将智能体重新概念化，不再是孤立的策略优化器，而是能够合成复杂多智能体动态并基于对未来交互的预测性理解做出前瞻性决策的复杂生成模型。生成式强化学习智能体（generative-RL agents）不再是对即时观测做出反应，而是能够建模环境演化、预测其他智能体的行为、生成协调的动作序列，并进行考虑长期动态的战略推理。该方法利用生成式AI的模式识别和生成能力，以实现主动决策、通过增强通信实现无缝协调，以及对不断变化的场景进行动态适应。我们预见，这种范式转变将为分布式智能开启前所未有的可能性，超越个体优化，走向代表真正协作智能的涌现集体行为。其影响将遍及自主系统、机器人技术和人机协作，有望为传统被动式框架下难以解决的协调挑战提供解决方案。</div>
                                <div style="margin-top: 15px;">
                                    <a href="https://arxiv.org/abs/2507.09495" target="_blank">查看摘要页面 (Abs Page)</a>
                                    <a href="javascript:void(0);" class="collapse-btn" style="margin-left: 20px;">折叠 (Collapse)</a>
                                </div>
                            </td>
                        </tr>
                        <tr class="paper-title-row">
                            <td>
                                <div class="paper-title">推理还是记忆？由于数据污染导致的强化学习结果不可靠</div>
                                <div><a href="https://arxiv.org/pdf/2507.10532" target="_blank" onclick="event.stopPropagation();" title="Download PDF">arXiv:2507.10532 - Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination</a></div>
                            </td>
                        </tr>
                        <tr class="details-row">
                            <td class="details-cell">
                                <strong>作者 (Authors):</strong>
                                <div class="authors">Mingqi Wu, Zhihao Zhang, Qiaole Dong, Zhiheng Xi, Jun Zhao, Senjie Jin, Xiaoran Fan, Yuhao Zhou, Yanwei Fu, Qin Liu, Songyang Zhang, Qi Zhang</div>
                                <strong style="margin-top:10px; display:block;">摘要 (Abstract):</strong>
                                <div class="abstract">大型语言模型（LLM）的推理能力长期以来一直是研究的焦点。最近的研究通过强化学习（RL）进一步增强了这些能力，许多新方法声称在很少或没有外部监督的情况下取得了显著的改进。令人惊讶的是，一些研究甚至表明随机或不正确的奖励信号可以提高推理性能。然而，这些突破主要在Qwen2.5模型家族上报告，并在MATH-500、AMC和AIME等著名基准上进行评估，却未能在Llama等其他模型上取得类似的增益，这值得进一步研究。我们的分析表明，尽管Qwen2.5在数学推理方面表现出色，但其在海量网络语料库上的预训练使其在流行基准中容易受到数据污染的影响。因此，基于这些基准得出的结果可能不可靠。为了解决这个问题，我们引入了一个生成器，可以生成任意长度和难度的完全合成的算术问题，从而产生一个我们称之为RandomCalculation的干净数据集。利用这些无泄漏的数据集，我们证明只有准确的奖励信号才能持续提高性能，而嘈杂或不正确的信号则不能。我们主张在未受污染的基准和多样化的模型家族上评估RL方法，以确保结论的可靠性。</div>
                                <div style="margin-top: 15px;">
                                    <a href="https://arxiv.org/abs/2507.10532" target="_blank">查看摘要页面 (Abs Page)</a>
                                    <a href="javascript:void(0);" class="collapse-btn" style="margin-left: 20px;">折叠 (Collapse)</a>
                                </div>
                            </td>
                        </tr>
                        <tr class="paper-title-row">
                            <td>
                                <div class="paper-title">通过在线世界模型规划实现持续强化学习</div>
                                <div><a href="https://arxiv.org/pdf/2507.09177" target="_blank" onclick="event.stopPropagation();" title="Download PDF">arXiv:2507.09177 - Continual Reinforcement Learning by Planning with Online World Models</a></div>
                            </td>
                        </tr>
                        <tr class="details-row">
                            <td class="details-cell">
                                <strong>作者 (Authors):</strong>
                                <div class="authors">Zichen Liu, Guoji Fu, Chao Du, Wee Sun Lee, Min Lin</div>
                                <strong style="margin-top:10px; display:block;">摘要 (Abstract):</strong>
                                <div class="abstract">持续强化学习（CRL）是一种自然主义的设定，其中智能体需要通过试错不断演化，以解决顺序呈现的多个任务。CRL的最大障碍之一是智能体在学习新任务时可能会忘记如何解决旧任务，这被称为灾难性遗忘。在本文中，我们提出通过在线世界模型规划来应对这一挑战。具体来说，我们在线学习一个“跟随领导者”（Follow-The-Leader）的浅层模型来捕捉世界动态，并在此模型中使用模型预测控制进行规划，以解决由任何奖励函数指定的一系列任务。该在线世界模型通过其构造在温和假设下具有O(√(K²Dlog(T)))的遗憾界，从而天然地免受遗忘的影响。规划器仅基于最新的在线模型搜索动作，从而形成一个增量更新的FTL在线智能体（OA）。为了评估OA，我们还设计了一个专门用于CRL的环境Continual Bench，并在相同的模型-规划算法框架下与几个强基线进行了比较。实证结果表明，OA能够持续学习解决新任务而不忘记旧技能，其性能优于使用各种持续学习技术构建的深度世界模型智能体。</div>
                                <div style="margin-top: 15px;">
                                    <a href="https://arxiv.org/abs/2507.09177" target="_blank">查看摘要页面 (Abs Page)</a>
                                    <a href="javascript:void(0);" class="collapse-btn" style="margin-left: 20px;">折叠 (Collapse)</a>
                                </div>
                            </td>
                        </tr>
                        <tr class="paper-title-row">
                            <td>
                                <div class="paper-title">我们应该在离线强化学习中优先选择决策转换器吗？</div>
                                <div><a href="https://arxiv.org/pdf/2507.10174" target="_blank" onclick="event.stopPropagation();" title="Download PDF">arXiv:2507.10174 - Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?</a></div>
                            </td>
                        </tr>
                        <tr class="details-row">
                            <td class="details-cell">
                                <strong>作者 (Authors):</strong>
                                <div class="authors">Yumi Omori, Zixuan Dong, Keith Ross</div>
                                <strong style="margin-top:10px; display:block;">摘要 (Abstract):</strong>
                                <div class="abstract">近年来，大量工作探索了将Transformer架构应用于强化学习问题。其中，决策转换器（Decision Transformer, DT）因其能将回报条件策略学习框架化为序列建模任务，在离线强化学习领域受到了特别关注。最近，Bhargava等人（2024）对DT与更传统的基于MLP的离线RL算法（包括行为克隆（BC）和保守Q学习（CQL））进行了系统比较，并声称DT在稀疏奖励和低质量数据设置下表现更优。本文中，我们通过在机器人操控任务（Robomimic）和运动基准（D4RL）上的实验表明，基于MLP的过滤行为克隆（Filtered Behavior Cloning, FBC）在稀疏奖励环境中与DT相比，取得了相当或更优的性能。FBC仅从数据集中过滤掉表现不佳的轨迹，然后在过滤后的数据集上执行普通行为克隆。FBC不仅非常直接，而且需要更少的训练数据和更高的计算效率。因此，这些结果表明DT在稀疏奖励环境中并非首选。根据先前的工作，DT在密集奖励环境中也可能不是首选。因此，我们提出一个问题：DT是否在任何情况下都是首选？</div>
                                <div style="margin-top: 15px;">
                                    <a href="https://arxiv.org/abs/2507.10174" target="_blank">查看摘要页面 (Abs Page)</a>
                                    <a href="javascript:void(0);" class="collapse-btn" style="margin-left: 20px;">折叠 (Collapse)</a>
                                </div>
                            </td>
                        </tr>
                        <tr class="paper-title-row">
                            <td>
                                <div class="paper-title">Prompt4Trust：一个用于多模态大型语言模型中临床对齐置信度校准的强化学习提示增强框架</div>
                                <div><a href="https://arxiv.org/pdf/2507.09279" target="_blank" onclick="event.stopPropagation();" title="Download PDF">arXiv:2507.09279 - Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models</a></div>
                            </td>
                        </tr>
                        <tr class="details-row">
                            <td class="details-cell">
                                <strong>作者 (Authors):</strong>
                                <div class="authors">Anita Kriz, Elizabeth Laura Janes, Xing Shen, Tal Arbel</div>
                                <strong style="margin-top:10px; display:block;">摘要 (Abstract):</strong>
                                <div class="abstract">多模态大型语言模型（MLLMs）在医疗保健应用中展现出巨大潜力。然而，它们在安全关键领域的部署受到两个关键限制的阻碍：（i）对提示设计的敏感性，以及（ii）倾向于以高置信度生成不正确的响应。由于临床医生可能依赖模型的置信度来评估其预测的可靠性，因此当模型表达高置信度时，其高度准确性尤为重要。我们介绍了Prompt4Trust，这是首个针对MLLM中置信度校准的强化学习（RL）提示增强框架。我们训练一个轻量级LLM来生成上下文感知的辅助提示，引导下游任务MLLM生成响应，使其表达的置信度更准确地反映预测准确性。与传统的校准技术不同，Prompt4Trust特别优先考虑对安全可靠的临床决策至关重要的校准方面。除了这个临床驱动的校准目标带来的改进外，我们提出的方法还提高了任务准确性，在PMC-VQA基准测试中实现了最先进的医疗视觉问答（VQA）性能，该基准由涵盖多种医疗成像模态的多项选择题组成。此外，我们用小型下游任务MLLM训练的框架在实验中对大型MLLM表现出有希望的零样本泛化能力，这表明了在不增加相关计算成本的情况下实现可扩展校准的潜力。这项工作展示了自动化但与人类对齐的提示工程在提高安全关键领域MLLM可信度方面的潜力。我们的代码库可在 https://this-url/ 上找到。</div>
                                <div style="margin-top: 15px;">
                                    <a href="https://arxiv.org/abs/2507.09279" target="_blank">查看摘要页面 (Abs Page)</a>
                                    <a href="javascript:void(0);" class="collapse-btn" style="margin-left: 20px;">折叠 (Collapse)</a>
                                </div>
                            </td>
                        </tr>
                        <tr class="paper-title-row">
                            <td>
                                <div class="paper-title">微型奖励模型</div>
                                <div><a href="https://arxiv.org/pdf/2507.09973" target="_blank" onclick="event.stopPropagation();" title="Download PDF">arXiv:2507.09973 - Tiny Reward Models</a></div>
                            </td>
                        </tr>
                        <tr class="details-row">
                            <td class="details-cell">
                                <strong>作者 (Authors):</strong>
                                <div class="authors">Sarah Pan</div>
                                <strong style="margin-top:10px; display:block;">摘要 (Abstract):</strong>
                                <div class="abstract">基于大型解码器的语言模型已成为从人类反馈中进行强化学习（RLHF）中奖励建模的主流架构。然而，随着奖励模型越来越多地部署在测试时策略中，其推理成本成为一个日益增长的担忧。我们提出了TinyRM，这是一系列小型的、双向的掩码语言模型（MLM），参数量少至4亿，但在推理和安全偏好建模任务上，其能力可与大175倍以上的模型相媲美。TinyRM结合了FLAN风格的提示、方向性低秩适应（DoRA）和层冻结，尽管使用的资源显著减少，但在RewardBench上取得了强大的性能。我们的实验表明，小型模型受益于领域特定的微调策略，特别是在推理方面，轻量级微调方法尤其有效。尽管在构建通用模型和对话偏好建模方面仍存在挑战，但我们的初步结果凸显了轻量级双向架构作为偏好建模的高效、可扩展替代方案的前景。</div>
                                <div style="margin-top: 15px;">
                                    <a href="https://arxiv.org/abs/2507.09973" target="_blank">查看摘要页面 (Abs Page)</a>
                                    <a href="javascript:void(0);" class="collapse-btn" style="margin-left: 20px;">折叠 (Collapse)</a>
                                </div>
                            </td>
                        </tr>
                            </tbody>
                        </table>
                    </section>
                    <section>
                        <h2>3D视觉与视频生成</h2>
                        <table>
                            <thead><tr><th>论文 (Paper)</th></tr></thead>
                            <tbody>
                        <tr class="paper-title-row">
                            <td>
                                <div class="paper-title">ScaffoldAvatar：利用分片表情的高保真高斯化身</div>
                                <div><a href="https://arxiv.org/pdf/2507.10542" target="_blank" onclick="event.stopPropagation();" title="Download PDF">arXiv:2507.10542 - ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions</a></div>
                            </td>
                        </tr>
                        <tr class="details-row">
                            <td class="details-cell">
                                <strong>作者 (Authors):</strong>
                                <div class="authors">Shivangi Aneja, Sebastian Weiss, Irene Baeza, Prashanth Chandran, Gaspard Zoss, Matthias Nießner, Derek Bradley</div>
                                <strong style="margin-top:10px; display:block;">摘要 (Abstract):</strong>
                                <div class="abstract">为沉浸式远程呈现和电影等众多图形应用生成高保真、实时的逼真3D头部化身动画序列至关重要。这是一个具有挑战性的问题，尤其是在渲染数字虚拟形象特写以展示角色的面部微观特征和表情时。为了捕捉人头富有表现力、细节丰富的特性，包括皮肤皱纹和更精细的面部运动，我们提出将局部定义的面部表情与3D高斯溅射（3D Gaussian splatting）相结合，以创建超高保真、富有表现力且逼真的3D头部化身。与以往在全局表情空间上操作的工作不同，我们将化身的动态性建立在基于分片的局部表情特征之上，并以分片级别合成3D高斯。具体来说，我们利用一个基于分片的几何3D人脸模型来提取分片表情，并通过将这些分片与最近的层次化场景表示Scaffold-GS的锚点耦合，学习如何将这些表情转化为局部的动态皮肤外观和运动。然后，这些锚点被用于即时合成3D高斯，并以分片表情和观察方向为条件。我们采用基于颜色的致密化和渐进式训练，以获得高质量的结果，并为高分辨率3K训练图像实现更快的收敛。通过利用分片级别的表情，ScaffoldAvatar在实时性、视觉上自然的运动以及涵盖多样的面部表情和风格方面，持续达到最先进的性能。</div>
                                <div style="margin-top: 15px;">
                                    <a href="https://arxiv.org/abs/2507.10542" target="_blank">查看摘要页面 (Abs Page)</a>
                                    <a href="javascript:void(0);" class="collapse-btn" style="margin-left: 20px;">折叠 (Collapse)</a>
                                </div>
                            </td>
                        </tr>
                        <tr class="paper-title-row">
                            <td>
                                <div class="paper-title">AirScape：一个具有运动可控性的空中生成式世界模型</div>
                                <div><a href="https://arxiv.org/pdf/2507.08885" target="_blank" onclick="event.stopPropagation();" title="Download PDF">arXiv:2507.08885 - AirScape: An Aerial Generative World Model with Motion Controllability</a></div>
                            </td>
                        </tr>
                        <tr class="details-row">
                            <td class="details-cell">
                                <strong>作者 (Authors):</strong>
                                <div class="authors">Baining Zhao, Rongze Tang, Mingyuan Jia, Ziyou Wang, Fanghang Man, Xin Zhang, Yu Shang, Weichen Zhang, Chen Gao, Wei Wu, Xin Wang, Xinlei Chen, Yong Li</div>
                                <strong style="margin-top:10px; display:block;">摘要 (Abstract):</strong>
                                <div class="abstract">如何使机器人能够在三维空间中预测其自身运动意图的后果，一直是具身智能领域的一个基本问题。为了探索更通用的空间想象能力，我们提出了AirScape，这是首个为六自由度空中智能体设计的世界模型。AirScape能够根据当前的视觉输入和运动意图预测未来的观测序列。具体来说，我们构建了一个用于空中世界模型训练和测试的数据集，其中包含11,000个视频-意图对。该数据集包括捕捉了各种无人机在广泛场景中动作的第一人称视角视频，并花费了超过1000小时对相应的运动意图进行标注。然后，我们开发了一个两阶段的训练方案，将一个最初没有具身空间知识的基础模型训练成一个受运动意图控制并遵循物理时空约束的世界模型。</div>
                                <div style="margin-top: 15px;">
                                    <a href="https://arxiv.org/abs/2507.08885" target="_blank">查看摘要页面 (Abs Page)</a>
                                    <a href="javascript:void(0);" class="collapse-btn" style="margin-left: 20px;">折叠 (Collapse)</a>
                                </div>
                            </td>
                        </tr>
                        <tr class="paper-title-row">
                            <td>
                                <div class="paper-title">ViTCoT：视频-文本交错的思维链以增强大型语言模型中的视频理解能力</div>
                                <div><a href="https://arxiv.org/pdf/2507.09876" target="_blank" onclick="event.stopPropagation();" title="Download PDF">arXiv:2507.09876 - ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models</a></div>
                            </td>
                        </tr>
                        <tr class="details-row">
                            <td class="details-cell">
                                <strong>作者 (Authors):</strong>
                                <div class="authors">Yongheng Zhang, Xu Liu, Ruihan Tao, Qiguang Chen, Hao Fei, Wanxiang Che, Libo Qin</div>
                                <strong style="margin-top:10px; display:block;">摘要 (Abstract):</strong>
                                <div class="abstract">视频理解在连接低层视觉信号与高层认知推理方面扮演着至关重要的角色，并且是自动驾驶、具身AI以及更广泛的通用人工智能（AGI）追求的基础。大型语言模型（LLM），特别是那些利用思维链（CoT）技术的模型，其快速发展极大地推动了视频推理能力的进步。然而，当前的方法主要依赖文本信息进行推理，在实际的视频推理过程中忽略了视觉模态。相比之下，人类在推理时会自然地重新审视视觉内容。受此启发，我们引入了一种新颖的视频推理范式：视频-文本交错思维链（ViTCoT），它促进了更直观、更符合认知习惯的推理过程。为此，我们首先构建了视频-文本交错基准（ViTIB），该基准使用多模态大语言模型（MLLM）进行关键视频选择并经过人工验证。此外，我们广泛探索了ViTCoT范式在视频理解领域的潜力。大量实验表明，与传统的纯文本CoT范式相比，ViTCoT显著提升了性能，并能有效激活MLLM中更多的神经元值。</div>
                                <div style="margin-top: 15px;">
                                    <a href="https://arxiv.org/abs/2507.09876" target="_blank">查看摘要页面 (Abs Page)</a>
                                    <a href="javascript:void(0);" class="collapse-btn" style="margin-left: 20px;">折叠 (Collapse)</a>
                                </div>
                            </td>
                        </tr>
                        <tr class="paper-title-row">
                            <td>
                                <div class="paper-title">无限视频理解</div>
                                <div><a href="https://arxiv.org/pdf/2507.09068" target="_blank" onclick="event.stopPropagation();" title="Download PDF">arXiv:2507.09068 - Infinite Video Understanding</a></div>
                            </td>
                        </tr>
                        <tr class="details-row">
                            <td class="details-cell">
                                <strong>作者 (Authors):</strong>
                                <div class="authors">Dell Zhang, Xiangyu Chen, Jixiang Luo, Mengxi Jia, Changzhi Sun, Ruilong Ren, Jingren Liu, Hao Sun, Xuelong Li</div>
                                <strong style="margin-top:10px; display:block;">摘要 (Abstract):</strong>
                                <div class="abstract">大型语言模型（LLM）及其多模态扩展（MLLM）的飞速发展在视频理解领域取得了显著进展。然而，一个根本性挑战依然存在：如何有效处理和理解超过数分钟或数小时的视频内容。尽管最近的研究，如Video-XL-2，展示了用于实现极致效率的新颖架构解决方案，而位置编码的进步，如HoPE和VideoRoPE++，旨在提升对长上下文的时空理解，但当前最先进的模型在处理长序列带来的海量视觉标记时，仍然面临严重的计算和内存限制。此外，尽管在像Deep Video Discovery这样的智能体推理系统方面取得了进展，但在长时间内保持时间连贯性、追踪复杂事件以及保留细粒度细节仍然是艰巨的挑战。本立场文件提出，多媒体研究的下一个合乎逻辑且雄心勃勃的前沿是“无限视频理解”——即模型能够持续处理、理解和推理任意长度、甚至可能永不结束的视频数据的能力。我们认为，将无限视频理解作为一个“蓝天”研究目标，为多媒体乃至更广泛的AI研究社区提供了至关重要的指路明灯，推动了在流式架构、持久记忆机制、层级与自适应表示、事件中心推理以及新型评估范式等领域的创新。借鉴近期在长/超长视频理解及几个密切相关领域的研究成果，我们概述了实现这一变革性能力的核心挑战和关键研究方向。</div>
                                <div style="margin-top: 15px;">
                                    <a href="https://arxiv.org/abs/2507.09068" target="_blank">查看摘要页面 (Abs Page)</a>
                                    <a href="javascript:void(0);" class="collapse-btn" style="margin-left: 20px;">折叠 (Collapse)</a>
                                </div>
                            </td>
                        </tr>
                        <tr class="paper-title-row">
                            <td>
                                <div class="paper-title">基于生成式AI的场景感知对话式高级驾驶辅助系统用于实时驾驶员辅助</div>
                                <div><a href="https://arxiv.org/pdf/2507.10500" target="_blank" onclick="event.stopPropagation();" title="Download PDF">arXiv:2507.10500 - Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance</a></div>
                            </td>
                        </tr>
                        <tr class="details-row">
                            <td class="details-cell">
                                <strong>作者 (Authors):</strong>
                                <div class="authors">Kyungtae Han, Yitao Chen, Rohit Gupta, Onur Altintas</div>
                                <strong style="margin-top:10px; display:block;">摘要 (Abstract):</strong>
                                <div class="abstract">虽然自动驾驶技术不断进步，但当前的高级驾驶辅助系统（ADAS）在解释场景上下文或通过自然语言与驾驶员互动方面仍然存在局限。这些系统通常依赖于预定义的逻辑，并且不支持基于对话的交互，这使得它们在动态环境或适应驾驶员意图时缺乏灵活性。本文提出了场景感知对话式ADAS（SC-ADAS），这是一个集成了生成式AI组件（包括大型语言模型、视觉到文本解释和结构化函数调用）的模块化框架，以实现实时、可解释和自适应的驾驶员辅助。SC-ADAS支持基于视觉和传感器上下文的多轮对话，允许通过自然语言提供建议和由驾驶员确认的ADAS控制。该系统在CARLA模拟器中实现，并使用基于云的生成式AI，将用户确认的意图作为结构化的ADAS命令执行，而无需进行模型微调。我们评估了SC-ADAS在场景感知、对话式和多轮交互中的表现，并强调了其中的权衡，例如基于视觉的上下文检索导致的延迟增加，以及对话历史累积导致的令牌增长。这些结果证明了将对话推理、场景感知和模块化ADAS控制相结合以支持下一代智能驾驶员辅助的可行性。</div>
                                <div style="margin-top: 15px;">
                                    <a href="https://arxiv.org/abs/2507.10500" target="_blank">查看摘要页面 (Abs Page)</a>
                                    <a href="javascript:void(0);" class="collapse-btn" style="margin-left: 20px;">折叠 (Collapse)</a>
                                </div>
                            </td>
                        </tr>
                            </tbody>
                        </table>
                    </section>
                    <section>
                        <h2>生成模型：理论与应用</h2>
                        <table>
                            <thead><tr><th>论文 (Paper)</th></tr></thead>
                            <tbody>
                        <tr class="paper-title-row">
                            <td>
                                <div class="paper-title">WildFX：一个由DAW驱动的用于真实世界音频效果图建模的流程</div>
                                <div><a href="https://arxiv.org/pdf/2507.10534" target="_blank" onclick="event.stopPropagation();" title="Download PDF">arXiv:2507.10534 - WildFX: A DAW-Powered Pipeline for In-the-Wild Audio FX Graph Modeling</a></div>
                            </td>
                        </tr>
                        <tr class="details-row">
                            <td class="details-cell">
                                <strong>作者 (Authors):</strong>
                                <div class="authors">Qihui Yang, Taylor Berg-Kirkpatrick, Julian McAuley, Zachary Novack</div>
                                <strong style="margin-top:10px; display:block;">摘要 (Abstract):</strong>
                                <div class="abstract">尽管端到端AI音乐生成取得了迅速进展，但对专业数字信号处理（DSP）工作流程的AI驱动建模仍然充满挑战。特别是，尽管人们对音频效果图（如混响、压缩、均衡）的神经黑盒建模兴趣日增，但基于AI的方法难以复制专业工作流程中细致的信号流和参数交互。现有的可微插件方法通常与真实世界的工具有所偏离，在同等计算约束下，其性能相对于简化的神经控制器较差。我们引入了WildFX，这是一个使用Docker容器化的流程，用于生成具有丰富效果图的多轨音频混合数据集，其后端由专业的数字音频工作站（DAW）驱动。WildFX支持无缝集成跨平台的商业插件或任何VST/VST3/LV2/CLAP格式的插件，从而实现结构复杂性（如侧链、分频）并达到高效的并行处理。一个极简的元数据接口简化了项目/插件的配置。实验通过对混合图、插件/增益参数的盲估计，展示了该流程的有效性，并证明了其在连接AI研究与实际DSP需求方面的能力。代码可在 https://this-url/ 上获取。</div>
                                <div style="margin-top: 15px;">
                                    <a href="https://arxiv.org/abs/2507.10534" target="_blank">查看摘要页面 (Abs Page)</a>
                                    <a href="javascript:void(0);" class="collapse-btn" style="margin-left: 20px;">折叠 (Collapse)</a>
                                </div>
                            </td>
                        </tr>
                        <tr class="paper-title-row">
                            <td>
                                <div class="paper-title">通过直接偏好优化使生成式语音增强与人类偏好对齐</div>
                                <div><a href="https://arxiv.org/pdf/2507.09929" target="_blank" onclick="event.stopPropagation();" title="Download PDF">arXiv:2507.09929 - Aligning Generative Speech Enhancement with Human Preferences via Direct Preference Optimization</a></div>
                            </td>
                        </tr>
                        <tr class="details-row">
                            <td class="details-cell">
                                <strong>作者 (Authors):</strong>
                                <div class="authors">Haoyang Li, Nana Hou, Yuchen Hu, Jixun Yao, Sabato Marco Siniscalchi, Eng Siong Chng</div>
                                <strong style="margin-top:10px; display:block;">摘要 (Abstract):</strong>
                                <div class="abstract">这项工作从语言模型（LM）的角度研究语音增强（SE）。我们提出了一种新颖的方法，利用直接偏好优化（DPO）来提高增强后语音的感知质量。使用UTMOS（一个神经MOS预测模型）作为人类评分的代理，我们的方法引导优化朝向感知上更优的输出。这与现有的基于LM的SE方法不同，后者专注于最大化干净语音标记的似然性，这可能与人类感知不符，并可能尽管预测误差低但导致质量下降。在2020年深度噪声抑制挑战赛测试集上的实验表明，将DPO应用于预训练的基于LM的SE模型，在各种语音质量指标上均取得了一致的提升，相对增益高达56%。据我们所知，这是DPO首次应用于SE，也是首次将代理感知反馈融入基于LM的SE训练中，为感知对齐的SE指明了一个有前途的方向。</div>
                                <div style="margin-top: 15px;">
                                    <a href="https://arxiv.org/abs/2507.09929" target="_blank">查看摘要页面 (Abs Page)</a>
                                    <a href="javascript:void(0);" class="collapse-btn" style="margin-left: 20px;">折叠 (Collapse)</a>
                                </div>
                            </td>
                        </tr>
                        <tr class="paper-title-row">
                            <td>
                                <div class="paper-title">AudioMAE++: 使用SwiGLU FFN学习更好的掩码音频表示</div>
                                <div><a href="https://arxiv.org/pdf/2507.10464" target="_blank" onclick="event.stopPropagation();" title="Download PDF">arXiv:2507.10464 - AudioMAE++: learning better masked audio representations with SwiGLU FFNs</a></div>
                            </td>
                        </tr>
                        <tr class="details-row">
                            <td class="details-cell">
                                <strong>作者 (Authors):</strong>
                                <div class="authors">Sarthak Yadav, Sergios Theodoridis, Zheng-Hua Tan</div>
                                <strong style="margin-top:10px; display:block;">摘要 (Abstract):</strong>
                                <div class="abstract">在音频频谱图补丁上训练的掩码自编码器（MAE）已成为学习自监督音频表示的一种主流方法。尽管最近的一些论文评估了在音频数据上训练MAE的关键方面，但这些方法大多数仍利用普通的Transformer构建模块，而Transformer社区已经稳步集成了更新的架构进展。在这项工作中，我们提出了AudioMAE++，一个改进的音频掩码自编码器，它具有两个这样的增强功能，即带有门控线性单元的macaron风格的Transformer块。当在AudioSet数据集上进行预训练时，所提出的AudioMAE++模型在10个不同的下游任务上优于现有的基于MAE的方法，在音频分类和基于语音的基准测试中表现出色。所提出的AudioMAE++模型还表现出优异的扩展特性，优于参数多达4倍的直接可比的标准MAE基线模型。</div>
                                <div style="margin-top: 15px;">
                                    <a href="https://arxiv.org/abs/2507.10464" target="_blank">查看摘要页面 (Abs Page)</a>
                                    <a href="javascript:void(0);" class="collapse-btn" style="margin-left: 20px;">折叠 (Collapse)</a>
                                </div>
                            </td>
                        </tr>
                        <tr class="paper-title-row">
                            <td>
                                <div class="paper-title">利用灵活表示指导学习扩散模型</div>
                                <div><a href="https://arxiv.org/pdf/2507.08980" target="_blank" onclick="event.stopPropagation();" title="Download PDF">arXiv:2507.08980 - Learning Diffusion Models with Flexible Representation Guidance</a></div>
                            </td>
                        </tr>
                        <tr class="details-row">
                            <td class="details-cell">
                                <strong>作者 (Authors):</strong>
                                <div class="authors">Chenyu Wang, Cai Zhou, Sharut Gupta, Zongyu Lin, Stefanie Jegelka, Stephen Bates, Tommi Jaakkola</div>
                                <strong style="margin-top:10px; display:block;">摘要 (Abstract):</strong>
                                <div class="abstract">扩散模型可以通过额外的指导来改进，使其朝向更有效的输入表示。事实上，先前的实证工作已经表明，将扩散模型的内部表示与预训练模型的表示对齐可以提高生成质量。在本文中，我们提出了一个系统性框架，用于将表示指导整合到扩散模型中。我们提供了去噪模型的替代分解及其相关的训练标准，其中分解决定了何时以及如何整合辅助表示。在我们的理论见解的指导下，我们引入了两种新策略来增强扩散模型中的表示对齐。首先，我们将样本与其目标表示配对，这些目标表示要么源自样本本身，要么来自不同的合成模态，然后学习多模态对的联合模型。其次，我们设计了一个最优的训练课程，以平衡表示学习和数据生成。我们在图像、蛋白质序列和分子生成任务上的实验表明，我们的方法性能优越，并且训练速度加快。特别是在类条件ImageNet 256x256基准测试上，我们的指导方法比原始的SiT-XL训练速度快23.3倍，比最先进的REPA方法快4倍。代码可在 https://this-url/ 上获取。</div>
                                <div style="margin-top: 15px;">
                                    <a href="https://arxiv.org/abs/2507.08980" target="_blank">查看摘要页面 (Abs Page)</a>
                                    <a href="javascript:void(0);" class="collapse-btn" style="margin-left: 20px;">折叠 (Collapse)</a>
                                </div>
                            </td>
                        </tr>
                        <tr class="paper-title-row">
                            <td>
                                <div class="paper-title">理论指导下对离散扩散模型无分类器指导的改进</div>
                                <div><a href="https://arxiv.org/pdf/2507.08965" target="_blank" onclick="event.stopPropagation();" title="Download PDF">arXiv:2507.08965 - Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models</a></div>
                            </td>
                        </tr>
                        <tr class="details-row">
                            <td class="details-cell">
                                <strong>作者 (Authors):</strong>
                                <div class="authors">Kevin Rojas, Ye He, Chieh-Hsin Lai, Yuta Takida, Yuki Mitsufuji, Molei Tao</div>
                                <strong style="margin-top:10px; display:block;">摘要 (Abstract):</strong>
                                <div class="abstract">无分类器指导（Classifier-Free Guidance, CFG）是一种广泛用于条件生成和提高连续扩散模型样本质量的技术，最近的研究已将其扩展到离散扩散模型。本文在掩码离散扩散的背景下，从理论上分析了CFG，重点关注指导时间表的作用。我们的分析表明，在采样早期（当输入被大量掩码时）进行强指导会损害生成质量，而后期指导则有更大的影响。这些发现为最近关于指导时间表研究中的经验观察提供了理论解释。分析还揭示了当前CFG实现的一个缺陷。这些实现可能无意中导致不平衡的转换，例如在生成的早期阶段过快地去掩码，从而降低了生成样本的质量。为了解决这个问题，我们从分析中汲取灵感，提出了一种新颖的无分类器指导机制，该机制在经验上适用于任何离散扩散模型。直观地说，我们的方法平滑了数据分布与初始（掩码/均匀）分布之间的传输过程，从而提高了样本质量。值得注意的是，我们的方法只需一行代码的简单更改即可实现。我们在ImageNet（掩码离散扩散）和QM9（均匀离散扩散）上的实验证明了我们方法的有效性。</div>
                                <div style="margin-top: 15px;">
                                    <a href="https://arxiv.org/abs/2507.08965" target="_blank">查看摘要页面 (Abs Page)</a>
                                    <a href="javascript:void(0);" class="collapse-btn" style="margin-left: 20px;">折叠 (Collapse)</a>
                                </div>
                            </td>
                        </tr>
                        <tr class="paper-title-row">
                            <td>
                                <div class="paper-title">AlphaVAE：具有Alpha感知表示学习的统一端到端RGBA图像重建与生成</div>
                                <div><a href="https://arxiv.org/pdf/2507.09308" target="_blank" onclick="event.stopPropagation();" title="Download PDF">arXiv:2507.09308 - AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning</a></div>
                            </td>
                        </tr>
                        <tr class="details-row">
                            <td class="details-cell">
                                <strong>作者 (Authors):</strong>
                                <div class="authors">Zile Wang, Hao Yu, Jiabo Zhan, Chun Yuan</div>
                                <strong style="margin-top:10px; display:block;">摘要 (Abstract):</strong>
                                <div class="abstract">近来，潜在扩散模型通过利用预训练的VAE（变分自编码器）在低计算成本下压缩和重建像素数据，在高质量RGB图像合成方面取得了显著成果。然而，由于缺乏大规模基准数据集，透明或分层内容（RGBA图像）的生成在很大程度上仍未被探索。在这项工作中，我们提出了ALPHA，这是首个全面的RGBA基准，它通过在标准背景上进行alpha混合，将标准的RGB度量标准应用于四通道图像。我们进一步引入了ALPHAVAE，一个统一的端到端RGBA VAE，它通过集成一个专用的alpha通道来扩展预训练的RGB VAE。该模型采用复合目标进行训练，结合了alpha混合像素重建、补丁级保真度、感知一致性以及双重KL散度约束，以确保RGB和alpha表示的潜在保真度。我们的RGBA VAE仅在8K图像上进行训练，而先前的方法使用了1M图像，但在重建方面，PSNR提高了+4.9 dB，SSIM提高了+3.2%。在潜在扩散框架内进行微调时，它还能实现更优越的透明图像生成。我们的代码、数据和模型已在 https://this-url/ 上发布，以供复现。</div>
                                <div style="margin-top: 15px;">
                                    <a href="https://arxiv.org/abs/2507.09308" target="_blank">查看摘要页面 (Abs Page)</a>
                                    <a href="javascript:void(0);" class="collapse-btn" style="margin-left: 20px;">折叠 (Collapse)</a>
                                </div>
                            </td>
                        </tr>
                            </tbody>
                        </table>
                    </section>
                    <section>
                        <h2>模型效率与架构创新</h2>
                        <table>
                            <thead><tr><th>论文 (Paper)</th></tr></thead>
                            <tbody>
                        <tr class="paper-title-row">
                            <td>
                                <div class="paper-title">DeepSeek：大型人工智能模型的范式转变与技术演进</div>
                                <div><a href="https://arxiv.org/pdf/2507.09955" target="_blank" onclick="event.stopPropagation();" title="Download PDF">arXiv:2507.09955 - DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models</a></div>
                            </td>
                        </tr>
                        <tr class="details-row">
                            <td class="details-cell">
                                <strong>作者 (Authors):</strong>
                                <div class="authors">Luolin Xiong, Haofen Wang, Xi Chen, Lu Sheng, Yun Xiong, Jingping Liu, Yanghua Xiao, Huajun Chen, Qing-Long Han, Yang Tang</div>
                                <strong style="margin-top:10px; display:block;">摘要 (Abstract):</strong>
                                <div class="abstract">中国人工智能（AI）初创公司DeepSeek发布了其V3和R1系列模型，因其低成本、高性能和开源优势而引起全球关注。本文首先回顾了大型AI模型的演变，重点关注范式转变、主流大型语言模型（LLM）范式以及DeepSeek范式。随后，本文重点介绍了DeepSeek引入的新颖算法，包括多头潜在注意力（MLA）、专家混合（MoE）、多标记预测（MTP）和组相对策略优化（GRPO）。接着，本文探讨了DeepSeek在LLM扩展、训练、推理和系统级优化架构方面的工程突破。此外，还分析了DeepSeek模型在竞争激烈的AI格局中的影响，将其与各个领域的主流LLM进行比较。最后，本文反思了从DeepSeek创新中获得的见解，并讨论了大型AI模型在技术和工程发展方面的未来趋势，特别是在数据、训练和推理方面。</div>
                                <div style="margin-top: 15px;">
                                    <a href="https://arxiv.org/abs/2507.09955" target="_blank">查看摘要页面 (Abs Page)</a>
                                    <a href="javascript:void(0);" class="collapse-btn" style="margin-left: 20px;">折叠 (Collapse)</a>
                                </div>
                            </td>
                        </tr>
                        <tr class="paper-title-row">
                            <td>
                                <div class="paper-title">生成性和判别性LSTM文本分类器的训练后量化：关于校准、类别平衡和鲁棒性的研究</div>
                                <div><a href="https://arxiv.org/pdf/2507.09687" target="_blank" onclick="event.stopPropagation();" title="Download PDF">arXiv:2507.09687 - Post-Training Quantization of Generative and Discriminative LSTM Text Classifiers: A Study of Calibration, Class Balance, and Robustness</a></div>
                            </td>
                        </tr>
                        <tr class="details-row">
                            <td class="details-cell">
                                <strong>作者 (Authors):</strong>
                                <div class="authors">Md Mushfiqur Rahaman, Elliot Chang, Tasmiah Haque, Srinjoy Das</div>
                                <strong style="margin-top:10px; display:block;">摘要 (Abstract):</strong>
                                <div class="abstract">文本分类在工业监控、健康诊断和智能助手等边缘计算应用中扮演着关键角色，这些应用对低延迟和高准确性都有要求。特别是，生成式分类器已被证明对分布外和噪声数据具有鲁棒性，这对于在此类实时边缘环境中的部署是一个极其重要的考量。然而，在边缘设备上部署此类模型面临计算和内存限制。训练后量化（PTQ）可以在不重新训练的情况下减小模型大小和计算成本，使其成为边缘部署的理想选择。在这项工作中，我们使用Brevitas量化库，对基于生成性和判别性长短期记忆（LSTM）的文本分类模型进行了全面的PTQ比较研究。我们评估了这两种分类器模型在多种位宽下的表现，并评估了它们在常规和噪声输入条件下的鲁棒性。我们发现，虽然判别性分类器保持鲁棒，但生成性分类器对位宽、PTQ期间使用的校准数据以及量化推理期间的输入噪声更为敏感。我们研究了校准数据中类别不平衡对两种分类器的影响，比较了类别样本均匀分布和不均匀分布的场景，包括它们在PTQ期间对权重调整和激活分布的影响。我们使用非参数假设检验得出的检验统计量，发现，在校准期间使用类别不平衡的数据会导致生成性LSTM分类器在较低位宽下权重适应不足，从而导致性能下降。这项研究强调了校准数据在PTQ中的作用，以及生成性分类器在噪声下成功或失败的条件，有助于在边缘环境中进行部署。</div>
                                <div style="margin-top: 15px;">
                                    <a href="https://arxiv.org/abs/2507.09687" target="_blank">查看摘要页面 (Abs Page)</a>
                                    <a href="javascript:void(0);" class="collapse-btn" style="margin-left: 20px;">折叠 (Collapse)</a>
                                </div>
                            </td>
                        </tr>
                        <tr class="paper-title-row">
                            <td>
                                <div class="paper-title">超越固定矩阵的差分隐私联邦低秩自适应</div>
                                <div><a href="https://arxiv.org/pdf/2507.09990" target="_blank" onclick="event.stopPropagation();" title="Download PDF">arXiv:2507.09990 - Differentially Private Federated Low Rank Adaptation Beyond Fixed-Matrix</a></div>
                            </td>
                        </tr>
                        <tr class="details-row">
                            <td class="details-cell">
                                <strong>作者 (Authors):</strong>
                                <div class="authors">Ming Wen, Jiaqi Zhu, Yuedong Xu, Yipeng Zhou, Dingding Han</div>
                                <strong style="margin-top:10px; display:block;">摘要 (Abstract):</strong>
                                <div class="abstract">大型语言模型（LLM）通常需要针对特定领域的任务进行微调，而LoRA通过训练低秩适配器提供了一种计算高效的方法。当多个用户协同微调一个全局LLM模型而不共享其专有原始数据时，LoRA对于联邦LLM也具有通信效率。然而，即使是服务器和客户端之间本地适配器的传输也存在严重的隐私泄露风险。将差分隐私（DP）应用于联邦LoRA会遇到一个两难问题：同时向两个适配器添加噪声会放大模型上的合成噪声，而固定一个适配器会损害微调的可学习性。在本文中，我们提出了FedASK（具有双重草图的差分隐私联邦低秩自适应），一个新颖的联邦LoRA框架，旨在以强大的差分隐私有效地更新两个低秩适配器。受随机SVD的启发，我们的关键思想是一个两阶段的草图管道。该管道首先聚合经过精心草图化、保护隐私的本地更新，然后在服务器上重建全局矩阵，以促进两个适配器的有效更新。我们从理论上证明了FedASK的差分隐私保证及其精确聚合属性。全面的实验表明，FedASK在各种隐私设置和数据分布下始终优于基线方法。</div>
                                <div style="margin-top: 15px;">
                                    <a href="https://arxiv.org/abs/2507.09990" target="_blank">查看摘要页面 (Abs Page)</a>
                                    <a href="javascript:void(0);" class="collapse-btn" style="margin-left: 20px;">折叠 (Collapse)</a>
                                </div>
                            </td>
                        </tr>
                            </tbody>
                        </table>
                    </section></div></div>
                    <script>
        document.addEventListener('DOMContentLoaded', () => {
            document.querySelectorAll('.paper-title-row').forEach(titleRow => {
                const detailsRow = titleRow.nextElementSibling;
                if (detailsRow) {
                    titleRow.addEventListener('click', () => {
                        titleRow.classList.toggle('expanded');
                        detailsRow.classList.toggle('show');
                    });
                    const collapseBtn = detailsRow.querySelector('.collapse-btn');
                    if (collapseBtn) {
                        collapseBtn.addEventListener('click', (e) => {
                            e.stopPropagation();
                            titleRow.classList.remove('expanded');
                            detailsRow.classList.remove('show');
                        });
                    }
                }
            });
        });</script>
                </body>
                </html>